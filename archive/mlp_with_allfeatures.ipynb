{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from rdkit import DataStructs\n",
    "from rdkit import Chem, DataStructs\n",
    "from rdkit.Chem import AllChem, Descriptors\n",
    "\n",
    "from typing import List, Union\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def morgan_binary_features_generator(mol: Union[str, Chem.Mol], plot_img = False,\n",
    "                                     radius: int = 6,\n",
    "                                     num_bits: int = 4096) -> np.ndarray:\n",
    "    \n",
    "    mol = Chem.MolFromSmiles(mol) if type(mol) == str else mol\n",
    "    if plot_img:\n",
    "        display(mol)\n",
    "    \n",
    "    features_vec = AllChem.GetHashedMorganFingerprint(mol, radius, nBits=num_bits)\n",
    "    features = np.zeros((1,), dtype=np.int8)\n",
    "    DataStructs.ConvertToNumpyArray(features_vec, features)\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMolDescriptors(mol: Union[str, Chem.Mol], missingVal=None):\n",
    "    ''' calculate the full list of descriptors for a molecule\n",
    "\n",
    "        missingVal is used if the descriptor cannot be calculated\n",
    "    '''\n",
    "    mol = Chem.MolFromSmiles(mol) if type(mol) == str else mol\n",
    "    res = {}\n",
    "    for nm,fn in Descriptors._descList:\n",
    "        # some of the descriptor fucntions can throw errors if they fail, catch those here:\n",
    "        try:\n",
    "            val = fn(mol)\n",
    "        except:\n",
    "            # print the error message:\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            # and set the descriptor value to whatever missingVal is\n",
    "            val = missingVal\n",
    "        res[nm] = val\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, min_delta=-1):\n",
    "        self.patience = patience  # number of times to allow for no improvement before stopping the execution\n",
    "        self.min_delta = min_delta  # the minimum change to be counted as improvement\n",
    "        self.counter = 0  # count the number of times the validation accuracy not improving\n",
    "        self.min_validation_loss = np.inf\n",
    "\n",
    "    # return True when encountering _patience_ times decrease in validation loss \n",
    "    def __call__(self, validation_loss, verbose=False):\n",
    "        if ((validation_loss+self.min_delta) < self.min_validation_loss):\n",
    "            self.min_validation_loss = validation_loss\n",
    "            self.counter = 0  # reset the counter if validation loss decreased at least by min_delta\n",
    "        elif ((validation_loss+self.min_delta) > self.min_validation_loss):\n",
    "            self.counter += 1 # increase the counter if validation loss is not decreased by the min_delta\n",
    "            if verbose:\n",
    "                print(f\"  >> now{validation_loss:.3f} > best{self.min_validation_loss:.3f}\")\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"../input/train.csv\")\n",
    "test_df = pd.read_csv(\"../input/test.csv\")\n",
    "\n",
    "train_df[\"AlogP\"].fillna(value=train_df[\"AlogP\"].mean(), inplace=True)\n",
    "test_df[\"AlogP\"].fillna(value=test_df[\"AlogP\"].mean(), inplace=True)\n",
    "train_df.dropna(axis=0, inplace=True)\n",
    "\n",
    "train_fps = pd.DataFrame(train_df[\"SMILES\"].apply(morgan_binary_features_generator).tolist())\n",
    "test_fps = pd.DataFrame(test_df[\"SMILES\"].apply(morgan_binary_features_generator).tolist())\n",
    "\n",
    "train_fps.rename(columns=lambda x: \"FPS_\" + str(x), inplace=True)\n",
    "test_fps.rename(columns=lambda x: \"FPS_\" + str(x), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_descriptor = pd.DataFrame([getMolDescriptors(smile) for smile in train_df['SMILES']])\n",
    "test_descriptor =  pd.DataFrame([getMolDescriptors(smile) for smile in test_df['SMILES']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.concat([train_df, train_descriptor], axis=1)\n",
    "test_df = pd.concat([test_df, test_descriptor], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.drop(columns=['AlogP', 'MolWt', 'NumHAcceptors', 'NumHDonors', 'NumRotatableBonds', 'MolLogP'], inplace=True)\n",
    "test_df.drop(columns=['AlogP', 'MolWt', 'NumHAcceptors', 'NumHDonors', 'NumRotatableBonds', 'MolLogP'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.fillna(train_df.mean(numeric_only=True), inplace=True)\n",
    "test_df.fillna(test_df.mean(numeric_only=True), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, tab_df, fps_df,  target: str, tab_scaler, fps_scaler, label_scaler=None, is_test=False):\n",
    "        self.tab_df = tab_df\n",
    "        self.fps_df = fps_df\n",
    "        self.target = target\n",
    "        self.is_test = is_test\n",
    "        self.tab_scaler = tab_scaler\n",
    "        self.fps_scaler = fps_scaler\n",
    "\n",
    "\n",
    "        if self.is_test:\n",
    "            self.drop_col = [\"id\", \"SMILES\"]\n",
    "            self.tab_features = self.tab_scaler[1].transform(self.tab_scaler[0].transform(self.tab_df.drop(columns = self.drop_col, axis=1)))\n",
    "            self.fps_features = self.fps_scaler[1].transform(self.fps_scaler[0].transform(self.fps_df))\n",
    "            #self.fps_features = self.fps_scaler.transform(self.fps_df)\n",
    "\n",
    "        else:\n",
    "            self.drop_col = [\"id\", \"SMILES\", \"MLM\", \"HLM\"]\n",
    "            self.tab_features = self.tab_scaler[1].fit_transform(self.tab_scaler[0].fit_transform(self.tab_df.drop(columns = self.drop_col, axis=1)))\n",
    "            self.fps_features = self.fps_scaler[1].fit_transform(self.fps_scaler[0].fit_transform(self.fps_df))\n",
    "            #self.fps_features = self.fps_scaler.fit_transform(self.fps_df)\n",
    "            if label_scaler is None:\n",
    "                self.label = self.tab_df[target].values.reshape(-1, 1)\n",
    "            else:\n",
    "                self.label = label_scaler.fit_transform(self.tab_df[[target]])\n",
    "\n",
    "            self.range_class = self.tab_df[target].apply(lambda x : np.int8(min(x, 100)//10)) # 구간 균등화 startify를 위함\n",
    "            \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        feature = np.concatenate([self.tab_features[index], self.fps_features[index]])\n",
    "        #feature = self.fps_features[index]\n",
    "\n",
    "        if self.is_test:\n",
    "            return torch.tensor(feature).float()\n",
    "        else:\n",
    "            label = self.label[index]\n",
    "            return torch.tensor(feature).float(), torch.tensor(label).float()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tab_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tab_scaler = [VarianceThreshold(threshold=0.01), MinMaxScaler()]\n",
    "fps_scaler = [VarianceThreshold(threshold=0.05), MinMaxScaler()]\n",
    "#fps_scaler = VarianceThreshold(threshold=0.05)\n",
    "label_scaler = None\n",
    "\n",
    "train_MLM = CustomDataset(tab_df = train_df, fps_df = train_fps, target=\"MLM\", tab_scaler = tab_scaler, fps_scaler=fps_scaler, label_scaler=label_scaler, is_test= False)\n",
    "test_MLM = CustomDataset(tab_df = test_df, fps_df = test_fps, target=\"MLM\", tab_scaler = tab_scaler, fps_scaler=fps_scaler, label_scaler=label_scaler, is_test= True)\n",
    "\n",
    "train_HLM = CustomDataset(tab_df = train_df, fps_df = train_fps, target=\"HLM\", tab_scaler = tab_scaler, fps_scaler=fps_scaler, label_scaler=label_scaler, is_test= False)\n",
    "test_HLM = CustomDataset(tab_df = test_df, fps_df = test_fps, target=\"HLM\", tab_scaler = tab_scaler, fps_scaler=fps_scaler, label_scaler=label_scaler, is_test= True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = train_MLM.fps_features.shape[1] + train_MLM.tab_features.shape[1] \n",
    "#input_size = train_MLM.fps_features.shape[1]\n",
    "print(input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter\n",
    "CFG = {'BATCH_SIZE': 256,\n",
    "       'EPOCHS': 8000,\n",
    "       'INPUT_SIZE': input_size,\n",
    "       'HIDDEN_SIZE': 1024,\n",
    "       'OUTPUT_SIZE': 1,\n",
    "       'DROPOUT_RATE': 0.8,\n",
    "       'LEARNING_RATE': 0.001}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_MLM.range_class.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train,valid split\n",
    "train_MLM_dataset, valid_MLM_dataset = train_test_split(train_MLM, test_size=0.2, random_state=42, stratify=train_MLM.range_class)\n",
    "train_HLM_dataset, valid_HLM_dataset = train_test_split(train_HLM, test_size=0.2, random_state=42, stratify=train_HLM.range_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_MLM_loader = DataLoader(dataset=train_MLM_dataset,\n",
    "                              batch_size=CFG['BATCH_SIZE'],\n",
    "                              shuffle=True)\n",
    "\n",
    "valid_MLM_loader = DataLoader(dataset=valid_MLM_dataset,\n",
    "                              batch_size=CFG['BATCH_SIZE'],\n",
    "                              shuffle=False)\n",
    "\n",
    "train_HLM_loader = DataLoader(dataset=train_HLM_dataset,\n",
    "                              batch_size=CFG['BATCH_SIZE'],\n",
    "                              shuffle=True)\n",
    "\n",
    "valid_HLM_loader = DataLoader(dataset=valid_HLM_dataset,\n",
    "                              batch_size=CFG['BATCH_SIZE'],\n",
    "                              shuffle=False)\n",
    "\n",
    "test_MLM_loader = DataLoader(dataset=test_MLM,\n",
    "                              batch_size=CFG['BATCH_SIZE'],\n",
    "                              shuffle=False)\n",
    "\n",
    "test_HLM_loader = DataLoader(dataset=test_HLM,\n",
    "                              batch_size=CFG['BATCH_SIZE'],\n",
    "                              shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, dropout_rate, out_size):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # fc 레이어 3개와 출력 레이어\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size) \n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc4 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc5 = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "        self.fc_out = nn.Linear(hidden_size, out_size)\n",
    "        \n",
    "        # 정규화\n",
    "        self.ln1 = nn.BatchNorm1d(hidden_size)\n",
    "        self.ln2 = nn.BatchNorm1d(hidden_size)\n",
    "        self.ln3 = nn.BatchNorm1d(hidden_size)        \n",
    "        self.ln4 = nn.BatchNorm1d(hidden_size)        \n",
    "        self.ln5 = nn.BatchNorm1d(hidden_size)        \n",
    "\n",
    "\n",
    "        # 활성화 함수\n",
    "        self.activation = nn.LeakyReLU()\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "     \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.ln1(out)\n",
    "        out = self.activation(out)\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        out = self.fc2(out)\n",
    "        out = self.ln2(out)\n",
    "        out = self.activation(out)\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        out = self.fc3(out)\n",
    "        out = self.ln3(out)\n",
    "        out = self.activation(out)\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        out = self.fc4(out)\n",
    "        out = self.ln4(out)\n",
    "        out = self.activation(out)\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        out = self.fc5(out)\n",
    "        out = self.ln5(out)\n",
    "        out = self.activation(out)\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        out = self.fc_out(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_MLM = Net(CFG['INPUT_SIZE'],CFG['HIDDEN_SIZE'],CFG['DROPOUT_RATE'],CFG['OUTPUT_SIZE']).to(\"cuda\")\n",
    "model_HLM = Net(CFG['INPUT_SIZE'],CFG['HIDDEN_SIZE'],CFG['DROPOUT_RATE'],CFG['OUTPUT_SIZE']).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_MLM)\n",
    "sum(p.numel() for p in model_MLM.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer_MLM = torch.optim.Adam(model_MLM.parameters(), lr=CFG['LEARNING_RATE'])\n",
    "scheduler_MLM = torch.optim.lr_scheduler.LambdaLR(optimizer = optimizer_MLM, lr_lambda= lambda epoch : 0.95**(epoch))\n",
    "optimizer_HLM = torch.optim.Adam(model_HLM.parameters(), lr=CFG['LEARNING_RATE'])\n",
    "scheduler_HLM = torch.optim.lr_scheduler.LambdaLR(optimizer = optimizer_HLM, lr_lambda= lambda epoch : 0.95**(epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, valid_loader, model, criterion, optimizer, scheduler,  epochs, label_scaling:Union[None, List] = None):\n",
    "\n",
    "    earlyStop = EarlyStopping(patience= 8, min_delta=-10)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0\n",
    "        for inputs, targets in train_loader:\n",
    "            optimizer.zero_grad() # Zero your gradients for every batch!\n",
    "            \n",
    "            output = model(inputs.to(\"cuda\"))\n",
    "            loss = criterion(output, targets.to(\"cuda\"))\n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step() # Adjust learning weights\n",
    "\n",
    "            if label_scaling is None:\n",
    "                running_loss += loss.item()\n",
    "\n",
    "            else:\n",
    "                metric_loss = label_scaling[1](label_scaling[0].inverse_transform(output.tolist()), targets.tolist())\n",
    "                running_loss += metric_loss\n",
    "            \n",
    "        \n",
    "        if epoch % 100 == 0:\n",
    "            model.eval()\n",
    "            valid_loss = 0\n",
    "            with torch.no_grad():\n",
    "                for inputs, targets in valid_loader:\n",
    "                    output = model(inputs.to(\"cuda\"))\n",
    "                    loss = criterion(output, targets.to(\"cuda\"))\n",
    "\n",
    "\n",
    "                    if label_scaling is None:\n",
    "                        valid_loss += loss.item()\n",
    "                    else:\n",
    "                        valid_metric_loss = label_scaling[1](label_scaling[0].inverse_transform(output.tolist()), targets.tolist())\n",
    "                        valid_loss += valid_metric_loss\n",
    "                    \n",
    "            print(f\"Epoch: {epoch:4d}/{epochs} with lr {scheduler.get_last_lr()[0]:.9f}, Train Loss: {np.sqrt(running_loss/len(train_loader))}, Valid Loss: {np.sqrt(valid_loss/len(valid_loader))}\")\n",
    "            \n",
    "            if earlyStop(valid_loss, verbose=True):\n",
    "                break\n",
    "\n",
    "            scheduler.step()    \n",
    "        \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_MLM = train(train_MLM_loader, valid_MLM_loader, model_MLM, criterion, optimizer_MLM, scheduler_MLM, epochs=CFG[\"EPOCHS\"], label_scaling=None)\n",
    "model_HLM = train(train_HLM_loader, valid_HLM_loader, model_HLM, criterion, optimizer_HLM, scheduler_HLM, epochs=CFG[\"EPOCHS\"], label_scaling=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(test_loader, model, label_scaler=None):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx, inputs in enumerate(test_loader):\n",
    "            print(f\"{idx:3d}th: \", end=\" \")\n",
    "            for d in inputs:\n",
    "                std = np.std(d.detach().numpy())\n",
    "                print(f\"{std} \", end=\" \")\n",
    "            print()\n",
    "            output = model(inputs.to(\"cuda\"))\n",
    "            if label_scaler is not None:\n",
    "                output = label_scaler.inverse_transform(output.cpu())\n",
    "            preds.extend(output.flatten().tolist())\n",
    "    \n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_MLM = inference(test_MLM_loader, model_MLM, label_scaler=label_scaler)\n",
    "predictions_HLM = inference(test_HLM_loader, model_HLM, label_scaler=label_scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('../input/sample_submission.csv')\n",
    "submission['MLM'] = predictions_MLM\n",
    "submission['HLM'] = predictions_HLM\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('../output/submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "smils",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
