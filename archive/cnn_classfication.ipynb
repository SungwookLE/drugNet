{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "from rdkit import DataStructs\n",
    "from rdkit import Chem, DataStructs\n",
    "from rdkit.Chem import PandasTools, AllChem, Descriptors\n",
    "from rdkit.ML.Descriptors.MoleculeDescriptors import MolecularDescriptorCalculator\n",
    "\n",
    "from typing import List, Union\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def morgan_binary_features_generator(mol: Union[str, Chem.Mol], plot_img = False,\n",
    "                                     radius: int = 2,\n",
    "                                     num_bits: int = 2048) -> np.ndarray:\n",
    "    \n",
    "    mol = Chem.MolFromSmiles(mol) if type(mol) == str else mol\n",
    "    if plot_img:\n",
    "        display(mol)\n",
    "    \n",
    "    features_vec = AllChem.GetMorganFingerprintAsBitVect(mol, radius, nBits=num_bits)\n",
    "    features = np.zeros((1,))\n",
    "    DataStructs.ConvertToNumpyArray(features_vec, features)\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMolDescriptors(mol: Union[str, Chem.Mol], missingVal=None):\n",
    "    ''' calculate the full list of descriptors for a molecule\n",
    "\n",
    "        missingVal is used if the descriptor cannot be calculated\n",
    "    '''\n",
    "    mol = Chem.MolFromSmiles(mol) if type(mol) == str else mol\n",
    "    res = {}\n",
    "    for nm,fn in Descriptors._descList:\n",
    "        # some of the descriptor fucntions can throw errors if they fail, catch those here:\n",
    "        try:\n",
    "            val = fn(mol)\n",
    "        except:\n",
    "            # print the error message:\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            # and set the descriptor value to whatever missingVal is\n",
    "            val = missingVal\n",
    "        res[nm] = val\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"../input/train.csv\")\n",
    "test_df = pd.read_csv(\"../input/test.csv\")\n",
    "\n",
    "train_df[\"AlogP\"].fillna(value=train_df[\"AlogP\"].mean(), inplace=True)\n",
    "test_df[\"AlogP\"].fillna(value=test_df[\"AlogP\"].mean(), inplace=True)\n",
    "train_df.dropna(axis=0, inplace=True)\n",
    "\n",
    "train_fps = pd.DataFrame(train_df[\"SMILES\"].apply(morgan_binary_features_generator).tolist())\n",
    "test_fps = pd.DataFrame(test_df[\"SMILES\"].apply(morgan_binary_features_generator).tolist())\n",
    "\n",
    "train_fps.rename(columns=lambda x: \"FPS_\" + str(x), inplace=True)\n",
    "test_fps.rename(columns=lambda x: \"FPS_\" + str(x), inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_descriptor = pd.DataFrame([getMolDescriptors(smile) for smile in train_df['SMILES']])\n",
    "test_descriptor =  pd.DataFrame([getMolDescriptors(smile) for smile in test_df['SMILES']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.concat([train_df, train_descriptor], axis=1)\n",
    "test_df = pd.concat([test_df, test_descriptor], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.fillna(train_df.mean(numeric_only=True), inplace=True)\n",
    "test_df.fillna(test_df.mean(numeric_only=True), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, tab_df, fps_df,  target: str, tab_scaler, fps_scaler, is_test=False):\n",
    "        self.tab_df = tab_df\n",
    "        self.fps_df = fps_df\n",
    "        self.target = target\n",
    "        self.is_test = is_test\n",
    "        self.tab_scaler = tab_scaler\n",
    "        self.fps_scaler = fps_scaler\n",
    "\n",
    "\n",
    "        if self.is_test:\n",
    "            self.drop_col = [\"id\", \"SMILES\"]\n",
    "            self.tab_features = self.tab_scaler[1].transform(self.tab_scaler[0].transform(self.tab_df.drop(columns = self.drop_col, axis=1)))\n",
    "            self.fps_features = self.fps_scaler[1].transform(self.fps_scaler[0].transform(self.fps_df))\n",
    "        else:\n",
    "            self.drop_col = [\"id\", \"SMILES\", \"MLM\", \"HLM\"]\n",
    "            self.tab_features = self.tab_scaler[1].fit_transform(self.tab_scaler[0].fit_transform(self.tab_df.drop(columns = self.drop_col, axis=1)))\n",
    "            self.fps_features = self.fps_scaler[1].fit_transform(self.fps_scaler[0].fit_transform(self.fps_df))\n",
    "            \n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        feature = np.concatenate([self.tab_features[index], self.fps_features[index]])\n",
    "        #feature = self.tab_features[index]\n",
    "\n",
    "\n",
    "        if self.is_test:\n",
    "            return torch.tensor(feature).float()\n",
    "        else:\n",
    "            label = self.tab_df[self.target][index]\n",
    "            return torch.tensor(feature).float(), torch.tensor(label).float().unsqueeze(dim=-1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tab_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tab_scaler = [VarianceThreshold(threshold=0.05), MinMaxScaler()]\n",
    "fps_scaler = [VarianceThreshold(threshold=0.05), MinMaxScaler()]\n",
    "\n",
    "train_MLM = CustomDataset(tab_df = train_df, fps_df = train_fps, target=\"MLM\", tab_scaler = tab_scaler, fps_scaler=fps_scaler , is_test= False)\n",
    "test_MLM = CustomDataset(tab_df = test_df, fps_df = test_fps, target=\"MLM\", tab_scaler = tab_scaler, fps_scaler=fps_scaler ,is_test= True)\n",
    "\n",
    "train_HLM = CustomDataset(tab_df = train_df, fps_df = train_fps, target=\"HLM\", tab_scaler = tab_scaler, fps_scaler=fps_scaler ,is_test= False)\n",
    "test_HLM = CustomDataset(tab_df = test_df, fps_df = test_fps, target=\"HLM\", tab_scaler = tab_scaler, fps_scaler=fps_scaler , is_test= True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "309\n"
     ]
    }
   ],
   "source": [
    "input_size = train_MLM.fps_features.shape[1] + train_MLM.tab_features.shape[1] \n",
    "#input_size = train_MLM.tab_features.shape[1]\n",
    "print(input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter\n",
    "CFG = {'BATCH_SIZE': 256,\n",
    "       'EPOCHS': 8000,\n",
    "       'INPUT_SIZE': input_size,\n",
    "       'HIDDEN_SIZE': 1024,\n",
    "       'OUTPUT_SIZE': 1,\n",
    "       'DROPOUT_RATE': 0.8,\n",
    "       'LEARNING_RATE': 0.0001}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=1, min_delta=0.0):\n",
    "        self.patience = patience  # number of times to allow for no improvement before stopping the execution\n",
    "        self.min_delta = min_delta  # the minimum change to be counted as improvement\n",
    "        self.counter = 0  # count the number of times the validation accuracy not improving\n",
    "        self.min_validation_loss = np.inf\n",
    "\n",
    "    # return True when encountering _patience_ times decrease in validation loss \n",
    "    def __call__(self, validation_loss):\n",
    "        if ((validation_loss+self.min_delta) < self.min_validation_loss):\n",
    "            self.min_validation_loss = validation_loss\n",
    "            self.counter = 0  # reset the counter if validation loss decreased at least by min_delta\n",
    "        elif ((validation_loss+self.min_delta) > self.min_validation_loss):\n",
    "            self.counter += 1 # increase the counter if validation loss is not decreased by the min_delta\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train,valid split\n",
    "train_MLM_dataset, valid_MLM_dataset = train_test_split(train_MLM, test_size=0.2, random_state=42)\n",
    "train_HLM_dataset, valid_HLM_dataset = train_test_split(train_HLM, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_MLM_loader = DataLoader(dataset=train_MLM_dataset,\n",
    "                              batch_size=CFG['BATCH_SIZE'],\n",
    "                              shuffle=True)\n",
    "\n",
    "valid_MLM_loader = DataLoader(dataset=valid_MLM_dataset,\n",
    "                              batch_size=CFG['BATCH_SIZE'],\n",
    "                              shuffle=False)\n",
    "\n",
    "\n",
    "train_HLM_loader = DataLoader(dataset=train_HLM_dataset,\n",
    "                              batch_size=CFG['BATCH_SIZE'],\n",
    "                              shuffle=True)\n",
    "\n",
    "valid_HLM_loader = DataLoader(dataset=valid_HLM_dataset,\n",
    "                              batch_size=CFG['BATCH_SIZE'],\n",
    "                              shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(model: nn.Module):\n",
    "    \"\"\"\n",
    "    Initializes the weights of a model in place.\n",
    "    :param model: An nn.Module.\n",
    "    \"\"\"\n",
    "    for param in model.parameters():\n",
    "        if param.dim() == 1:\n",
    "            nn.init.constant_(param, 0)\n",
    "        else:\n",
    "            nn.init.xavier_normal_(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, dropout_rate, out_size):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # fc 레이어 3개와 출력 레이어\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size) \n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc4 = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "        self.fc_out = nn.Linear(hidden_size, out_size)\n",
    "        \n",
    "        # 정규화\n",
    "        self.ln1 = nn.LayerNorm(hidden_size)\n",
    "        self.ln2 = nn.LayerNorm(hidden_size)\n",
    "        self.ln3 = nn.LayerNorm(hidden_size)        \n",
    "        self.ln4 = nn.LayerNorm(hidden_size)        \n",
    "\n",
    "        \n",
    "        # 활성화 함수\n",
    "        self.activation = nn.LeakyReLU()\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "     \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.ln1(out)\n",
    "        out = self.activation(out)\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        out = self.fc2(out)\n",
    "        out = self.ln2(out)\n",
    "        out = self.activation(out)\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        out = self.fc3(out)\n",
    "        out = self.ln3(out)\n",
    "        out = self.activation(out)\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        out = self.fc4(out)\n",
    "        out = self.ln4(out)\n",
    "        out = self.activation(out)\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        out = self.fc_out(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_MLM = Net(CFG['INPUT_SIZE'],CFG['HIDDEN_SIZE'],CFG['DROPOUT_RATE'],CFG['OUTPUT_SIZE']).to(\"cuda\")\n",
    "model_HLM = Net(CFG['INPUT_SIZE'],CFG['HIDDEN_SIZE'],CFG['DROPOUT_RATE'],CFG['OUTPUT_SIZE']).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "initialize_weights(model_MLM)\n",
    "initialize_weights(model_HLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (fc1): Linear(in_features=309, out_features=1024, bias=True)\n",
       "  (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  (fc3): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  (fc4): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  (fc_out): Linear(in_features=1024, out_features=1, bias=True)\n",
       "  (ln1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  (ln2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  (ln3): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  (ln4): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  (activation): LeakyReLU(negative_slope=0.01)\n",
       "  (dropout): Dropout(p=0.8, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_MLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer_MLM = torch.optim.Adam(model_MLM.parameters(), lr=CFG['LEARNING_RATE'])\n",
    "scheduler_MLM = torch.optim.lr_scheduler.LambdaLR(optimizer = optimizer_MLM, lr_lambda= lambda epoch : 0.95**(epoch))\n",
    "optimizer_HLM = torch.optim.Adam(model_HLM.parameters(), lr=CFG['LEARNING_RATE'])\n",
    "scheduler_HLM = torch.optim.lr_scheduler.LambdaLR(optimizer = optimizer_HLM, lr_lambda= lambda epoch : 0.95**(epoch))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, valid_loader, model, criterion, optimizer, scheduler, epochs):\n",
    "    model.train()\n",
    "    earlyStop = EarlyStopping(patience= 3, min_delta=1)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0\n",
    "        for inputs, targets in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            output = model(inputs.to(\"cuda\"))\n",
    "            loss = criterion(output, targets.to(\"cuda\"))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "        \n",
    "        if epoch % 100 == 0:\n",
    "            model.eval()\n",
    "            valid_loss = 0\n",
    "            with torch.no_grad():\n",
    "                for inputs, targets in valid_loader:\n",
    "                    output = model(inputs.to(\"cuda\"))\n",
    "                    loss = criterion(output, targets.to(\"cuda\"))\n",
    "                    valid_loss += loss.item()\n",
    "                    \n",
    "            print(f'Epoch: {epoch}/{epochs} with lr {scheduler.get_last_lr()[0]:.9f}, Train Loss: {np.sqrt(running_loss/len(train_loader))}, Valid Loss: {np.sqrt(valid_loss/len(valid_HLM_loader))}')\n",
    "            if earlyStop(valid_loss):\n",
    "                break\n",
    "\n",
    "            model.train()\n",
    "            scheduler.step()    \n",
    "        \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Start: MLM\n",
      "Epoch: 0/1000 with lr 0.000100000, Train Loss: 51.47840270919502, Valid Loss: 52.603925071639935\n",
      "Epoch: 100/1000 with lr 0.000095000, Train Loss: 43.069307825233786, Valid Loss: 44.234277953060854\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Start: MLM\")\n",
    "model_MLM = train(train_MLM_loader, valid_MLM_loader, model_MLM, criterion, optimizer_MLM, scheduler_MLM, epochs=CFG['EPOCHS'])\n",
    "\n",
    "print(\"Training Start: HLM\")\n",
    "model_HLM = train(train_HLM_loader, valid_HLM_loader, model_HLM, criterion, optimizer_HLM, scheduler_HLM, epochs=CFG['EPOCHS'])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "smils",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
