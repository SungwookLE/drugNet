{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9fts0HCHvD5p",
        "outputId": "c139e089-d5f2-48c0-e506-a5841d5a5cfe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rdkit\n",
            "  Downloading rdkit-2023.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m29.7/29.7 MB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rdkit) (1.23.5)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from rdkit) (9.4.0)\n",
            "Installing collected packages: rdkit\n",
            "Successfully installed rdkit-2023.3.3\n"
          ]
        }
      ],
      "source": [
        "!pip install rdkit"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import PandasTools\n",
        "from rdkit.Chem import AllChem\n",
        "import rdkit\n",
        "from rdkit.Chem.Draw import IPythonConsole\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.model_selection import train_test_split\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from rdkit.Chem import Descriptors"
      ],
      "metadata": {
        "id": "OaSLxNYTvKTW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def getMolDescriptors(mol, missingVal=None):\n",
        "    ''' calculate the full list of descriptors for a molecule\n",
        "\n",
        "        missingVal is used if the descriptor cannot be calculated\n",
        "    '''\n",
        "    res = {}\n",
        "    for nm,fn in Descriptors._descList:\n",
        "        # some of the descriptor fucntions can throw errors if they fail, catch those here:\n",
        "        try:\n",
        "            val = fn(mol)\n",
        "        except:\n",
        "            # print the error message:\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "            # and set the descriptor value to whatever missingVal is\n",
        "            val = missingVal\n",
        "        res[nm] = val\n",
        "    return res"
      ],
      "metadata": {
        "id": "hU3W341pvOt5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv(\"/content/drive/MyDrive/metabolism_dacon/train.csv\")\n",
        "train[\"AlogP\"] = np.where(pd.isna(train[\"AlogP\"]), train[\"LogD\"], train[\"AlogP\"])\n",
        "\n",
        "test = pd.read_csv(\"/content/drive/MyDrive/metabolism_dacon/test.csv\")\n",
        "test[\"AlogP\"] = np.where(pd.isna(test[\"AlogP\"]), test[\"LogD\"], test[\"AlogP\"])\n",
        "\n",
        "full = pd.concat([train, test], axis = 0).reset_index(drop=True)\n",
        "full['Molecule'] = full['SMILES'].apply(Chem.MolFromSmiles)"
      ],
      "metadata": {
        "id": "5X5M2oVhvQ0L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from rdkit import DataStructs\n",
        "\n",
        "allDescrs = [getMolDescriptors(m) for m in full['Molecule']]\n",
        "full_Descrs = pd.DataFrame(allDescrs)\n",
        "\n",
        "# SMILES로부터 분자 객체 생성\n",
        "full['Molecule'] = full['SMILES'].apply(Chem.MolFromSmiles)\n",
        "\n",
        "# 화합물의 특성 벡터 생성 (여기서는 Morgan fingerprint 사용)\n",
        "full['Fingerprint'] = full['Molecule'].apply(lambda mol: AllChem.GetMorganFingerprintAsBitVect(mol, 2, nBits=1024))\n",
        "\n",
        "fps = list(full['Fingerprint'])\n",
        "distance_matrix = []\n",
        "for i in range(len(full)):\n",
        "    similarities = []\n",
        "    for j in range(len(full)):\n",
        "        sim = DataStructs.TanimotoSimilarity(fps[i], fps[j])\n",
        "        similarities.append(sim)\n",
        "    distance_matrix.append(similarities)\n",
        "\n",
        "# distance matrix를 데이터프레임으로 변환\n",
        "distance_df = pd.DataFrame(distance_matrix, columns=full.index, index=full.index)"
      ],
      "metadata": {
        "id": "vd5IqBqTvSTu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Data = pd.concat([full, full_Descrs], axis=1).drop(columns=['AlogP', 'Molecule', 'MolWt', 'NumHAcceptors', 'NumHDonors', 'NumRotatableBonds', 'MolLogP'])\n",
        "\n",
        "train = Data.iloc[:3498].dropna(axis=0)\n",
        "test = Data.iloc[3498:]"
      ],
      "metadata": {
        "id": "0bM0oAI7vVK6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#결측 확인\n",
        "for col in train.columns:\n",
        "    has_missing_values = train[col].isna().any()\n",
        "    count =  train[col].isna().sum()\n",
        "    if has_missing_values:\n",
        "        print(f\"Column '{col}' has missing values.\")\n",
        "        print(f\"'{count}'\")"
      ],
      "metadata": {
        "id": "E44JTyLzvWm4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for col in test.columns:\n",
        "    has_missing_values = test[col].isna().any()\n",
        "    count =  test[col].isna().sum()\n",
        "    if has_missing_values:\n",
        "        print(f\"Column '{col}' has missing values.\")\n",
        "        print(f\"'{count}'\")"
      ],
      "metadata": {
        "id": "OMGoITWovYmT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 숫자형식 컬럼들의 min-max 정규화\n",
        "scaler = MinMaxScaler()\n",
        "non_features = ['id', 'SMILES', 'MLM', 'HLM','Fingerprint']\n",
        "features = [column for column in train.columns if column not in non_features]\n",
        "train[features] = scaler.fit_transform(train[features])\n",
        "\n",
        "test[features] = scaler.transform(test[features])"
      ],
      "metadata": {
        "id": "Byn8IAnfvaGO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, BatchNormalization, Dropout\n",
        "from keras.optimizers import Adam\n",
        "from keras.metrics import RootMeanSquaredError\n",
        "import keras\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import math\n",
        "\n",
        "# Define features and targets\n",
        "non_features = ['id', 'SMILES', 'MLM', 'HLM', 'Fingerprint']\n",
        "features = [column for column in train.columns if column not in non_features]\n",
        "mlm_target = \"MLM\"\n",
        "hlm_target = \"HLM\"\n",
        "\n",
        "# Initialize KFold\n",
        "seed = 42\n",
        "n_splits = 10\n",
        "kf = KFold(n_splits=n_splits, random_state=seed, shuffle=True)\n",
        "\n",
        "# Initialize arrays to store models and scores\n",
        "reg_mlms = []\n",
        "reg_hlms = []\n",
        "\n",
        "# Initialize arrays to store RMSE scores\n",
        "mlm_rmse_scores = []\n",
        "hlm_rmse_scores = []\n",
        "\n",
        "# Loop through KFold splits\n",
        "for i, (train_index, valid_index) in enumerate(kf.split(train)):\n",
        "    df_train = train.iloc[train_index]\n",
        "    df_valid = train.iloc[valid_index]\n",
        "\n",
        "    x_train = df_train[features].values\n",
        "    y_mlm_train = df_train[mlm_target].values\n",
        "    y_hlm_train = df_train[hlm_target].values\n",
        "\n",
        "    x_valid = df_valid[features].values\n",
        "    y_mlm_valid = df_valid[mlm_target].values\n",
        "    y_hlm_valid = df_valid[hlm_target].values\n",
        "\n",
        "    # Create and compile the model\n",
        "    model = Sequential()\n",
        "    model.add(Dense(512, activation='relu', input_shape=(x_train.shape[1],)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(1024, activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(512, activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(256, activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(1))\n",
        "\n",
        "    model.compile(optimizer=Adam(), loss='mean_squared_error', metrics=[RootMeanSquaredError()])\n",
        "\n",
        "    # Create and compile another model for HLM\n",
        "    model_hlm = Sequential()\n",
        "    model_hlm.add(Dense(512, activation='relu', input_shape=(x_train.shape[1],)))\n",
        "    model_hlm.add(BatchNormalization())\n",
        "    model_hlm.add(Dropout(0.2))\n",
        "    model_hlm.add(Dense(1024, activation='relu'))\n",
        "    model_hlm.add(BatchNormalization())\n",
        "    model_hlm.add(Dropout(0.2))\n",
        "    model_hlm.add(Dense(512, activation='relu'))\n",
        "    model_hlm.add(BatchNormalization())\n",
        "    model_hlm.add(Dropout(0.2))\n",
        "    model_hlm.add(Dense(256, activation='relu'))\n",
        "    model_hlm.add(BatchNormalization())\n",
        "    model_hlm.add(Dropout(0.2))\n",
        "    model_hlm.add(Dense(1))\n",
        "\n",
        "    model_hlm.compile(optimizer=Adam(), loss='mean_squared_error', metrics=[RootMeanSquaredError()])\n",
        "\n",
        "\n",
        "    # Train the model\n",
        "    checkpoint_mlm = ModelCheckpoint(f'model_mlm_fold{i}.h5', monitor='val_loss', verbose=0, save_best_only=True)\n",
        "    early_stopping_mlm = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
        "\n",
        "    model.fit(x_train, y_mlm_train, epochs=500, batch_size=32, verbose=1, validation_data=(x_valid, y_mlm_valid),\n",
        "              callbacks=[checkpoint_mlm, early_stopping_mlm])\n",
        "\n",
        "    reg_mlms.append(model)\n",
        "\n",
        "    checkpoint_hlm = ModelCheckpoint(f'model_hlm_fold{i}.h5', monitor='val_loss', verbose=0, save_best_only=True)\n",
        "    early_stopping_hlm = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
        "\n",
        "    model_hlm.fit(x_train, y_hlm_train, epochs=500, batch_size=32, verbose=1, validation_data=(x_valid, y_hlm_valid),\n",
        "                  callbacks=[checkpoint_hlm, early_stopping_hlm])\n",
        "\n",
        "    reg_hlms.append(model_hlm)\n",
        "\n",
        "    # Calculate RMSE for MLM predictions\n",
        "    y_mlm_pred = model.predict(x_valid)\n",
        "    mlm_rmse = math.sqrt(mean_squared_error(y_mlm_valid, y_mlm_pred))\n",
        "    print(mlm_rmse)\n",
        "    mlm_rmse_scores.append(mlm_rmse)\n",
        "\n",
        "    # Calculate RMSE for HLM predictions\n",
        "    y_hlm_pred = model_hlm.predict(x_valid)\n",
        "    hlm_rmse = math.sqrt(mean_squared_error(y_hlm_valid, y_hlm_pred))\n",
        "    print(hlm_rmse)\n",
        "    hlm_rmse_scores.append(hlm_rmse)"
      ],
      "metadata": {
        "id": "Ik0Syr5Cvbqj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import load_model\n",
        "\n",
        "# Load MLM models\n",
        "mlm_models = [load_model(f'model_mlm_fold{i}.h5') for i in range(n_splits)]\n",
        "\n",
        "# Load HLM models\n",
        "hlm_models = [load_model(f'model_hlm_fold{i}.h5') for i in range(n_splits)]\n",
        "\n",
        "x_test = test[features].values\n",
        "\n",
        "mlm_predictions = []\n",
        "hlm_predictions = []\n",
        "\n",
        "for model_mlm, model_hlm in zip(mlm_models, hlm_models):\n",
        "    mlm_predictions.append(model_mlm.predict(x_test))\n",
        "    hlm_predictions.append(model_hlm.predict(x_test))\n",
        "\n",
        "# Convert prediction lists to numpy arrays\n",
        "mlm_predictions = np.array(mlm_predictions)\n",
        "hlm_predictions = np.array(hlm_predictions)\n",
        "\n",
        "mlm_ensemble_prediction = mlm_predictions.mean(axis=0)\n",
        "hlm_ensemble_prediction = hlm_predictions.mean(axis=0)"
      ],
      "metadata": {
        "id": "Kfq4gATZviZd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_submission = pd.read_csv(\"/content/drive/MyDrive/metabolism_dacon/sample_submission.csv\")\n",
        "df_submission[\"MLM\"] = mlm_ensemble_prediction\n",
        "df_submission[\"HLM\"] = hlm_ensemble_prediction\n",
        "df_submission.to_csv(\"submission.csv\", index = False, encoding = \"utf-8-sig\")"
      ],
      "metadata": {
        "id": "9Wu_M0nmvi8q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load data\n",
        "df = test\n",
        "\n",
        "# predict\n",
        "df_submission = pd.read_csv(\"/content/drive/MyDrive/metabolism_dacon/sample_submission.csv\")\n",
        "df_submission[\"MLM\"] = np.mean([reg_mlm.predict(df[features].values) for reg_mlm in reg_mlms], axis = 0)\n",
        "df_submission[\"HLM\"] = np.mean([reg_hlm.predict(df[features].values) for reg_hlm in reg_hlms], axis = 0)\n",
        "df_submission.to_csv(\"submission.csv\", index = False, encoding = \"utf-8-sig\")"
      ],
      "metadata": {
        "id": "fX0HHRMlvkkr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}