{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from rdkit import DataStructs\n",
    "from rdkit import Chem, DataStructs\n",
    "from rdkit.Chem import AllChem, Descriptors\n",
    "\n",
    "from typing import List, Union\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def morgan_binary_features_generator(mol: Union[str, Chem.Mol], plot_img = False,\n",
    "                                     radius: int = 6,\n",
    "                                     num_bits: int = 4096) -> np.ndarray:\n",
    "    \n",
    "    mol = Chem.MolFromSmiles(mol) if type(mol) == str else mol\n",
    "    if plot_img:\n",
    "        display(mol)\n",
    "    \n",
    "    features_vec = AllChem.GetHashedMorganFingerprint(mol, radius, nBits=num_bits)\n",
    "    features = np.zeros((1,), dtype=np.int8)\n",
    "    DataStructs.ConvertToNumpyArray(features_vec, features)\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, min_delta : float=-1.0):\n",
    "        self.patience = patience  # number of times to allow for no improvement before stopping the execution\n",
    "        self.min_delta = min_delta  # the minimum change to be counted as improvement\n",
    "        self.counter = 0  # count the number of times the validation accuracy not improving\n",
    "        self.min_validation_loss = np.inf\n",
    "\n",
    "    # return True when encountering _patience_ times decrease in validation loss \n",
    "    def __call__(self, validation_loss, verbose=False):\n",
    "        if ((validation_loss+self.min_delta) < self.min_validation_loss):\n",
    "            self.min_validation_loss = validation_loss\n",
    "            self.counter = 0  # reset the counter if validation loss decreased at least by min_delta\n",
    "        elif ((validation_loss+self.min_delta) > self.min_validation_loss):\n",
    "            self.counter += 1 # increase the counter if validation loss is not decreased by the min_delta\n",
    "            if verbose:\n",
    "                print(f\"  >> now{validation_loss:.3f} > best{self.min_validation_loss:.3f}\")\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4096"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(\"../input/train.csv\")\n",
    "test_df = pd.read_csv(\"../input/test.csv\")\n",
    "\n",
    "train_fps = pd.DataFrame(train_df[\"SMILES\"].apply(morgan_binary_features_generator).tolist())\n",
    "test_fps = pd.DataFrame(test_df[\"SMILES\"].apply(morgan_binary_features_generator).tolist())\n",
    "\n",
    "train_fps.rename(columns=lambda x: \"FPS_\" + str(x), inplace=True)\n",
    "test_fps.rename(columns=lambda x: \"FPS_\" + str(x), inplace=True)\n",
    "\n",
    "fps_inputsize = train_fps.shape[1]\n",
    "fps_inputsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FPsDataset(Dataset):\n",
    "    def __init__(self, fps_df, scaler=None):\n",
    "        if scaler is not None:\n",
    "            self.fps_df = scaler.fit_transform(fps_df)\n",
    "        else:\n",
    "            self.fps_df = fps_df.values\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        feature = self.fps_df[index]\n",
    "        return torch.tensor(feature).float()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.fps_df)\n",
    "    \n",
    "FPs_dataset = FPsDataset(train_fps)\n",
    "train_FPs_dataset, valid_FPs_dataset = train_test_split(FPs_dataset, test_size=0.2, random_state=42)\n",
    "\n",
    "train_FPs_loader = DataLoader(dataset=train_FPs_dataset, batch_size=256, shuffle=True)\n",
    "valid_FPs_loader = DataLoader(dataset=valid_FPs_dataset, batch_size=256, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FpsAutoEncoder(\n",
      "  (encoder): Sequential(\n",
      "    (0): Linear(in_features=4096, out_features=512, bias=True)\n",
      "    (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.2, inplace=False)\n",
      "    (4): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU()\n",
      "    (7): Dropout(p=0.2, inplace=False)\n",
      "    (8): Linear(in_features=256, out_features=32, bias=True)\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): Linear(in_features=32, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=256, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=4096, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class FpsAutoEncoder(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(FpsAutoEncoder, self).__init__()\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_size, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, output_size)\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(output_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, input_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.encoder(x)\n",
    "        out = self.decoder(out)\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def get_codes(self, x):\n",
    "        return self.encoder(x)\n",
    "    \n",
    "model_auto = FpsAutoEncoder(fps_inputsize, 32).to(\"cuda\")\n",
    "print(model_auto)\n",
    "\n",
    "criterion_auto = nn.MSELoss()\n",
    "optimizer_auto = torch.optim.Adam(model_auto.parameters(), lr=0.0001)\n",
    "scheduler_auto = torch.optim.lr_scheduler.LambdaLR(optimizer = optimizer_auto, lr_lambda= lambda epoch : 0.95**(epoch))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:    0/5000, lr: 0.0000017, Train Loss: 0.12449290529529422, Valid Loss: 0.13595647387159018\n",
      "Epoch:  100/5000, lr: 0.0000016, Train Loss: 0.12446421534738933, Valid Loss: 0.13593135660912675\n",
      "Epoch:  200/5000, lr: 0.0000015, Train Loss: 0.12452852574300592, Valid Loss: 0.1362083398855532\n",
      "Epoch:  300/5000, lr: 0.0000014, Train Loss: 0.12445314290880831, Valid Loss: 0.13605875840633033\n",
      "Epoch:  400/5000, lr: 0.0000013, Train Loss: 0.1244362883125623, Valid Loss: 0.13595841930097147\n",
      "Epoch:  500/5000, lr: 0.0000013, Train Loss: 0.12441203160763148, Valid Loss: 0.1360053911436378\n",
      "Epoch:  600/5000, lr: 0.0000012, Train Loss: 0.1243931829974885, Valid Loss: 0.13623808198025708\n",
      "Epoch:  700/5000, lr: 0.0000012, Train Loss: 0.12452265203551856, Valid Loss: 0.13599368794019034\n",
      "Epoch:  800/5000, lr: 0.0000011, Train Loss: 0.12449416276508528, Valid Loss: 0.1360067606744744\n",
      "Epoch:  900/5000, lr: 0.0000010, Train Loss: 0.12436739807392537, Valid Loss: 0.13587015509909275\n",
      "Epoch: 1000/5000, lr: 0.0000010, Train Loss: 0.12449257137338313, Valid Loss: 0.13636292976078318\n",
      "Epoch: 1100/5000, lr: 0.0000009, Train Loss: 0.12437633967647455, Valid Loss: 0.135952742770684\n",
      "Epoch: 1200/5000, lr: 0.0000009, Train Loss: 0.12437337271502384, Valid Loss: 0.13604597362221021\n",
      "Epoch: 1300/5000, lr: 0.0000008, Train Loss: 0.12438026431622116, Valid Loss: 0.13583888582156498\n",
      "Epoch: 1400/5000, lr: 0.0000008, Train Loss: 0.12440145067140991, Valid Loss: 0.13618635331538137\n",
      "Epoch: 1500/5000, lr: 0.0000008, Train Loss: 0.12427761659377916, Valid Loss: 0.13594393979819405\n",
      "Epoch: 1600/5000, lr: 0.0000007, Train Loss: 0.12440579102280304, Valid Loss: 0.13616309328591708\n",
      "Epoch: 1700/5000, lr: 0.0000007, Train Loss: 0.12438970833459671, Valid Loss: 0.13585466303596008\n",
      "Epoch: 1800/5000, lr: 0.0000007, Train Loss: 0.12451363968344874, Valid Loss: 0.13613991365378608\n",
      "Epoch: 1900/5000, lr: 0.0000006, Train Loss: 0.12441636104210754, Valid Loss: 0.13602410002482399\n",
      "Epoch: 2000/5000, lr: 0.0000006, Train Loss: 0.12427810403619281, Valid Loss: 0.13598625050696048\n",
      "Epoch: 2100/5000, lr: 0.0000006, Train Loss: 0.12436088701190037, Valid Loss: 0.1361681842504412\n",
      "Epoch: 2200/5000, lr: 0.0000005, Train Loss: 0.12444563929465509, Valid Loss: 0.1361361123282102\n",
      "Epoch: 2300/5000, lr: 0.0000005, Train Loss: 0.12432951966649622, Valid Loss: 0.13609388014564175\n",
      "Epoch: 2400/5000, lr: 0.0000005, Train Loss: 0.1243285962566705, Valid Loss: 0.13603088498170632\n",
      "Epoch: 2500/5000, lr: 0.0000005, Train Loss: 0.12435174582872022, Valid Loss: 0.1358063703879406\n",
      "Epoch: 2600/5000, lr: 0.0000004, Train Loss: 0.1243859667526267, Valid Loss: 0.1362324695143993\n",
      "Epoch: 2700/5000, lr: 0.0000004, Train Loss: 0.12434330734795208, Valid Loss: 0.13593027636456026\n",
      "Epoch: 2800/5000, lr: 0.0000004, Train Loss: 0.12428075819641729, Valid Loss: 0.13593706372070458\n",
      "Epoch: 2900/5000, lr: 0.0000004, Train Loss: 0.12445052610068731, Valid Loss: 0.13617224229451944\n",
      "Epoch: 3000/5000, lr: 0.0000004, Train Loss: 0.12432632549736561, Valid Loss: 0.13608791727515565\n",
      "Epoch: 3100/5000, lr: 0.0000003, Train Loss: 0.12430657709764255, Valid Loss: 0.1359726255716873\n",
      "Epoch: 3200/5000, lr: 0.0000003, Train Loss: 0.12434234148377901, Valid Loss: 0.13623957677585916\n",
      "Epoch: 3300/5000, lr: 0.0000003, Train Loss: 0.12429174245165305, Valid Loss: 0.13604635697784587\n",
      "Epoch: 3400/5000, lr: 0.0000003, Train Loss: 0.12435317289565737, Valid Loss: 0.1359966760393086\n",
      "Epoch: 3500/5000, lr: 0.0000003, Train Loss: 0.1242773219473679, Valid Loss: 0.13589567659166182\n",
      "Epoch: 3600/5000, lr: 0.0000003, Train Loss: 0.12438602460948986, Valid Loss: 0.13620523335246307\n",
      "Epoch: 3700/5000, lr: 0.0000002, Train Loss: 0.1243825749320738, Valid Loss: 0.13621675657039073\n",
      "Epoch: 3800/5000, lr: 0.0000002, Train Loss: 0.12439712650317199, Valid Loss: 0.13596318228873286\n",
      "Epoch: 3900/5000, lr: 0.0000002, Train Loss: 0.12433770781885456, Valid Loss: 0.13611150033834604\n",
      "Epoch: 4000/5000, lr: 0.0000002, Train Loss: 0.12435553337548762, Valid Loss: 0.13595602404416748\n",
      "Epoch: 4100/5000, lr: 0.0000002, Train Loss: 0.12430091393065619, Valid Loss: 0.13596053138468694\n",
      "Epoch: 4200/5000, lr: 0.0000002, Train Loss: 0.12438737164579089, Valid Loss: 0.13599260134241445\n",
      "Epoch: 4300/5000, lr: 0.0000002, Train Loss: 0.12433558022448271, Valid Loss: 0.13580922088520672\n",
      "Epoch: 4400/5000, lr: 0.0000002, Train Loss: 0.12445880901294606, Valid Loss: 0.136101777011171\n",
      "Epoch: 4500/5000, lr: 0.0000002, Train Loss: 0.12430926334221372, Valid Loss: 0.13585740740775112\n",
      "Epoch: 4600/5000, lr: 0.0000002, Train Loss: 0.12444926818997934, Valid Loss: 0.13600941295997945\n",
      "Epoch: 4700/5000, lr: 0.0000001, Train Loss: 0.12422921402197014, Valid Loss: 0.1362163303927841\n",
      "Epoch: 4800/5000, lr: 0.0000001, Train Loss: 0.12431548765998585, Valid Loss: 0.13608204767578008\n",
      "Epoch: 4900/5000, lr: 0.0000001, Train Loss: 0.12440616158753645, Valid Loss: 0.1359932199737252\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FpsAutoEncoder(\n",
       "  (encoder): Sequential(\n",
       "    (0): Linear(in_features=4096, out_features=512, bias=True)\n",
       "    (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Dropout(p=0.2, inplace=False)\n",
       "    (4): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): ReLU()\n",
       "    (7): Dropout(p=0.2, inplace=False)\n",
       "    (8): Linear(in_features=256, out_features=32, bias=True)\n",
       "  )\n",
       "  (decoder): Sequential(\n",
       "    (0): Linear(in_features=32, out_features=256, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=256, out_features=512, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=512, out_features=4096, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def train_autoencoder(model, train_fps_loader, valid_fps_loader, criterion, optimizer, scheduler, epochs=5000):\n",
    "\n",
    "    earlyStopper = EarlyStopping(patience=5, min_delta=-0.1)\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0\n",
    "        for inputs in train_fps_loader:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output = model(inputs.to(\"cuda\"))\n",
    "            loss = criterion(output, inputs.to(\"cuda\"))\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "\n",
    "        if epoch % 100 == 0:\n",
    "            model.eval()\n",
    "            valid_loss = 0\n",
    "            with torch.no_grad():\n",
    "                for inputs in valid_fps_loader:\n",
    "                    output = model(inputs.to(\"cuda\"))\n",
    "                    loss = criterion(output, inputs.to(\"cuda\"))\n",
    "\n",
    "                    valid_loss += loss.item()\n",
    "            \n",
    "\n",
    "            print(f\"Epoch: {epoch:4d}/{epochs}, lr: {scheduler.get_last_lr()[0]:.7f}, Train Loss: {np.sqrt(running_loss/len(train_fps_loader))}, Valid Loss: {np.sqrt(valid_loss/len(valid_fps_loader))}\")\n",
    "            scheduler.step()\n",
    "\n",
    "            if earlyStopper(valid_loss, True):\n",
    "                break\n",
    "\n",
    "    return model\n",
    "\n",
    "train_autoencoder(model_auto, train_FPs_loader, valid_FPs_loader, criterion_auto, optimizer_auto, scheduler_auto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_auto.state_dict(), \"../archive_model/autoEncoder.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = FpsAutoEncoder(fps_inputsize, 32).to(\"cuda\")\n",
    "model.load_state_dict(torch.load('../archive_model/autoEncoder.pt', map_location=\"cuda:0\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "smils",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
