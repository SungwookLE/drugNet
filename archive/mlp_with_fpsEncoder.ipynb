{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "from rdkit import DataStructs\n",
    "from rdkit import Chem, DataStructs\n",
    "from rdkit.Chem import AllChem, Descriptors\n",
    "\n",
    "from typing import List, Union\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = dict()\n",
    "for idx, (nm,fn) in enumerate(Descriptors._descList):\n",
    "    d[idx] = nm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(d, index=[0]).to_clipboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def morgan_binary_features_generator(mol: Union[str, Chem.Mol], plot_img = False,\n",
    "                                     radius: int = 6,\n",
    "                                     num_bits: int = 4096) -> np.ndarray:\n",
    "    \n",
    "    mol = Chem.MolFromSmiles(mol) if type(mol) == str else mol\n",
    "    if plot_img:\n",
    "        display(mol)\n",
    "    \n",
    "    features_vec = AllChem.GetHashedMorganFingerprint(mol, radius, nBits=num_bits)\n",
    "    features = np.zeros((1,), dtype=np.int8)\n",
    "    DataStructs.ConvertToNumpyArray(features_vec, features)\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMolDescriptors(mol: Union[str, Chem.Mol], missingVal=None):\n",
    "    ''' calculate the full list of descriptors for a molecule\n",
    "\n",
    "        missingVal is used if the descriptor cannot be calculated\n",
    "    '''\n",
    "    mol = Chem.MolFromSmiles(mol) if type(mol) == str else mol\n",
    "    res = {}\n",
    "    for nm,fn in Descriptors._descList:\n",
    "        # some of the descriptor fucntions can throw errors if they fail, catch those here:\n",
    "        try:\n",
    "            val = fn(mol)\n",
    "        except:\n",
    "            # print the error message:\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            # and set the descriptor value to whatever missingVal is\n",
    "            val = missingVal\n",
    "        res[nm] = val\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, min_delta=-1, save_best:bool=False):\n",
    "        self.patience = patience  # number of times to allow for no improvement before stopping the execution\n",
    "        self.min_delta = min_delta  # the minimum change to be counted as improvement: if it is negative means, lower value is good\n",
    "        self.counter = 0  # count the number of times the validation accuracy not improving\n",
    "        self.min_validation_loss = np.inf\n",
    "        self.save_best = save_best\n",
    "\n",
    "    # return True when encountering _patience_ times decrease in validation loss \n",
    "    def __call__(self, validation_loss, verbose=False):\n",
    "        if ((validation_loss+self.min_delta) < self.min_validation_loss):\n",
    "            self.min_validation_loss = validation_loss\n",
    "            self.counter = 0  # reset the counter if validation loss decreased at least by min_delta\n",
    "            if self.save_best is True:\n",
    "                torch.save(\"../archive_model/saved_model.pt\", map_location = \"cuda:0\")\n",
    "            \n",
    "        elif ((validation_loss+self.min_delta) > self.min_validation_loss):\n",
    "            self.counter += 1 # increase the counter if validation loss is not decreased by the min_delta\n",
    "            if verbose:\n",
    "                print(f\"  >> now{validation_loss:.3f} > best{self.min_validation_loss:.3f}\")\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"../input/train.csv\")\n",
    "test_df = pd.read_csv(\"../input/test.csv\")\n",
    "\n",
    "train_df[\"AlogP\"].fillna(value=train_df[\"AlogP\"].mean(), inplace=True)\n",
    "test_df[\"AlogP\"].fillna(value=test_df[\"AlogP\"].mean(), inplace=True)\n",
    "train_df.dropna(axis=0, inplace=True)\n",
    "train_df.drop_duplicates([\"SMILES\"], inplace=True)\n",
    "train_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "train_fps = pd.DataFrame(train_df[\"SMILES\"].apply(morgan_binary_features_generator).tolist())\n",
    "test_fps = pd.DataFrame(test_df[\"SMILES\"].apply(morgan_binary_features_generator).tolist())\n",
    "\n",
    "train_fps.rename(columns=lambda x: \"FPS_\" + str(x), inplace=True)\n",
    "test_fps.rename(columns=lambda x: \"FPS_\" + str(x), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_descriptor = pd.DataFrame([getMolDescriptors(smile) for smile in train_df['SMILES']])\n",
    "test_descriptor =  pd.DataFrame([getMolDescriptors(smile) for smile in test_df['SMILES']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.concat([train_df, train_descriptor], axis=1)\n",
    "test_df = pd.concat([test_df, test_descriptor], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.drop(columns=['AlogP', 'MolWt', 'NumHAcceptors', 'NumHDonors', 'NumRotatableBonds', 'MolLogP'], axis=1, inplace=True)\n",
    "test_df.drop(columns=['AlogP', 'MolWt', 'NumHAcceptors', 'NumHDonors', 'NumRotatableBonds', 'MolLogP'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.fillna(train_df.mean(numeric_only=True), inplace=True)\n",
    "test_df.fillna(test_df.mean(numeric_only=True), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, tab_df, fps_df,  target: str, tab_scaler, label_scaler=None, is_test=False):\n",
    "        self.tab_df = tab_df\n",
    "        self.fps_df = fps_df\n",
    "        self.target = target\n",
    "        self.is_test = is_test\n",
    "        self.tab_scaler = tab_scaler\n",
    "\n",
    "\n",
    "        if self.is_test:\n",
    "            self.drop_col = [\"id\", \"SMILES\"]\n",
    "            self.tab_features = self.tab_scaler[1].transform(self.tab_scaler[0].transform(self.tab_df.drop(columns = self.drop_col, axis=1)))\n",
    "            self.fps_features = self.fps_df.values\n",
    "\n",
    "        else:\n",
    "            self.drop_col = [\"id\", \"SMILES\", \"MLM\", \"HLM\"]\n",
    "            self.tab_features = self.tab_scaler[1].fit_transform(self.tab_scaler[0].fit_transform(self.tab_df.drop(columns = self.drop_col, axis=1)))\n",
    "            self.fps_features = self.fps_df.values\n",
    "\n",
    "            if label_scaler is None:\n",
    "                self.label = self.tab_df[target].values.reshape(-1, 1)\n",
    "            else:\n",
    "                self.label = label_scaler.fit_transform(self.tab_df[[target]])\n",
    "\n",
    "            self.range_class = self.tab_df[target].apply(lambda x : np.int8(min(x, 100)//10)) # 구간 균등화 startify를 위함\n",
    "            \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        tab_feautres = self.tab_features[index]\n",
    "        fps_feautres = self.fps_features[index]\n",
    "\n",
    "        if self.is_test:\n",
    "            return torch.tensor(tab_feautres).float(), torch.tensor(fps_feautres).float()\n",
    "        else:\n",
    "            label = self.label[index]\n",
    "            return torch.tensor(tab_feautres).float(), torch.tensor(fps_feautres).float(), torch.tensor(label).float()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tab_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tab_scaler = [VarianceThreshold(threshold=0), MinMaxScaler()]\n",
    "label_scaler = None\n",
    "\n",
    "train_MLM = CustomDataset(tab_df = train_df, fps_df = train_fps, target=\"MLM\", tab_scaler = tab_scaler,  label_scaler=label_scaler, is_test= False)\n",
    "test_MLM = CustomDataset(tab_df = test_df, fps_df = test_fps, target=\"MLM\", tab_scaler = tab_scaler, label_scaler=label_scaler, is_test= True)\n",
    "\n",
    "train_HLM = CustomDataset(tab_df = train_df, fps_df = train_fps, target=\"HLM\", tab_scaler = tab_scaler, label_scaler=label_scaler, is_test= False)\n",
    "test_HLM = CustomDataset(tab_df = test_df, fps_df = test_fps, target=\"HLM\", tab_scaler = tab_scaler, label_scaler=label_scaler, is_test= True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "224\n"
     ]
    }
   ],
   "source": [
    "input_size = train_MLM.tab_features.shape[1] + 32 \n",
    "#input_size = train_MLM.fps_features.shape[1]\n",
    "print(input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter\n",
    "CFG = {'BATCH_SIZE': 256,\n",
    "       'EPOCHS': 8000,\n",
    "       'INPUT_SIZE': input_size,\n",
    "       'HIDDEN_SIZE': 1024,\n",
    "       'OUTPUT_SIZE': 1,\n",
    "       'DROPOUT_RATE': 0.8,\n",
    "       'LEARNING_RATE': 0.001}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     1333\n",
       "9      407\n",
       "1      253\n",
       "8      241\n",
       "7      239\n",
       "2      219\n",
       "6      199\n",
       "4      198\n",
       "5      190\n",
       "3      189\n",
       "10       3\n",
       "Name: MLM, dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_MLM.range_class.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train,valid split\n",
    "train_MLM_dataset, valid_MLM_dataset = train_test_split(train_MLM, test_size=0.2, random_state=42, stratify=train_MLM.range_class)\n",
    "train_HLM_dataset, valid_HLM_dataset = train_test_split(train_HLM, test_size=0.2, random_state=42, stratify=train_HLM.range_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_MLM_loader = DataLoader(dataset=train_MLM_dataset,\n",
    "                              batch_size=CFG['BATCH_SIZE'],\n",
    "                              shuffle=True)\n",
    "\n",
    "valid_MLM_loader = DataLoader(dataset=valid_MLM_dataset,\n",
    "                              batch_size=CFG['BATCH_SIZE'],\n",
    "                              shuffle=False)\n",
    "\n",
    "train_HLM_loader = DataLoader(dataset=train_HLM_dataset,\n",
    "                              batch_size=CFG['BATCH_SIZE'],\n",
    "                              shuffle=True)\n",
    "\n",
    "valid_HLM_loader = DataLoader(dataset=valid_HLM_dataset,\n",
    "                              batch_size=CFG['BATCH_SIZE'],\n",
    "                              shuffle=False)\n",
    "\n",
    "test_MLM_loader = DataLoader(dataset=test_MLM,\n",
    "                              batch_size=CFG['BATCH_SIZE'],\n",
    "                              shuffle=False)\n",
    "\n",
    "test_HLM_loader = DataLoader(dataset=test_HLM,\n",
    "                              batch_size=CFG['BATCH_SIZE'],\n",
    "                              shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FpsAutoEncoder(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(FpsAutoEncoder, self).__init__()\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_size, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, output_size)\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(output_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, input_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.encoder(x)\n",
    "        out = self.decoder(out)\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def get_codes(self, x):\n",
    "        return self.encoder(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder.0.weight: torch.Size([512, 4096])\n",
      "encoder.0.bias: torch.Size([512])\n",
      "encoder.1.weight: torch.Size([512])\n",
      "encoder.1.bias: torch.Size([512])\n",
      "encoder.4.weight: torch.Size([256, 512])\n",
      "encoder.4.bias: torch.Size([256])\n",
      "encoder.5.weight: torch.Size([256])\n",
      "encoder.5.bias: torch.Size([256])\n",
      "encoder.8.weight: torch.Size([32, 256])\n",
      "encoder.8.bias: torch.Size([32])\n",
      "decoder.0.weight: torch.Size([256, 32])\n",
      "decoder.0.bias: torch.Size([256])\n",
      "decoder.2.weight: torch.Size([512, 256])\n",
      "decoder.2.bias: torch.Size([512])\n",
      "decoder.4.weight: torch.Size([4096, 512])\n",
      "decoder.4.bias: torch.Size([4096])\n"
     ]
    }
   ],
   "source": [
    "model_fps_encoder = FpsAutoEncoder(4096, 32)\n",
    "model_fps_encoder.load_state_dict(torch.load(\"../archive_model/autoEncoder.pt\"))\n",
    "\n",
    "for name, param in model_fps_encoder.named_parameters():\n",
    "    print(f\"{name}: {param.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, dropout_rate, out_size, encoder):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.fps_encoder = encoder\n",
    "        \n",
    "        for p in self.fps_encoder.parameters():\n",
    "            p.requires_grad = False ## 모델 freeze\n",
    "        \n",
    "        # fc 레이어 3개와 출력 레이어\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size) \n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "        self.fc_out = nn.Linear(hidden_size + input_size, out_size)\n",
    "        \n",
    "        # 정규화\n",
    "        self.ln1 = nn.BatchNorm1d(hidden_size)\n",
    "        self.ln2 = nn.BatchNorm1d(hidden_size)\n",
    "\n",
    "        # 활성화 함수\n",
    "        self.activation = nn.LeakyReLU()\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "     \n",
    "    def forward(self, tab_x, fps_x):\n",
    "\n",
    "        enc = self.fps_encoder.get_codes(fps_x)\n",
    "\n",
    "        x = torch.cat([enc, tab_x], dim=1)        \n",
    "\n",
    "        out1 = self.fc1(x)\n",
    "        out1 = self.ln1(out1)\n",
    "        out1 = self.activation(out1)\n",
    "        out1 = self.dropout(out1)\n",
    "        \n",
    "        out2 = self.fc2(out1)\n",
    "        out2 = self.ln2(out2)\n",
    "        out2 = self.activation(out2)\n",
    "        out2 = self.dropout(out2)\n",
    "\n",
    "        out3 = torch.cat([x , out2], dim=1)\n",
    "        \n",
    "        out = self.fc_out(out3)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_MLM = Net(CFG['INPUT_SIZE'],CFG['HIDDEN_SIZE'],CFG['DROPOUT_RATE'],CFG['OUTPUT_SIZE'], model_fps_encoder).to(\"cuda\")\n",
    "model_HLM = Net(CFG['INPUT_SIZE'],CFG['HIDDEN_SIZE'],CFG['DROPOUT_RATE'],CFG['OUTPUT_SIZE'], model_fps_encoder).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fps_encoder): FpsAutoEncoder(\n",
      "    (encoder): Sequential(\n",
      "      (0): Linear(in_features=4096, out_features=512, bias=True)\n",
      "      (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "      (3): Dropout(p=0.2, inplace=False)\n",
      "      (4): Linear(in_features=512, out_features=256, bias=True)\n",
      "      (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (6): ReLU()\n",
      "      (7): Dropout(p=0.2, inplace=False)\n",
      "      (8): Linear(in_features=256, out_features=32, bias=True)\n",
      "    )\n",
      "    (decoder): Sequential(\n",
      "      (0): Linear(in_features=32, out_features=256, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=256, out_features=512, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear(in_features=512, out_features=4096, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (fc1): Linear(in_features=224, out_features=1024, bias=True)\n",
      "  (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (fc_out): Linear(in_features=1248, out_features=1, bias=True)\n",
      "  (ln1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (ln2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (activation): LeakyReLU(negative_slope=0.01)\n",
      "  (dropout): Dropout(p=0.8, inplace=False)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1285345"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(model_MLM)\n",
    "sum(p.numel() for p in model_MLM.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer_MLM = torch.optim.Adam(model_MLM.parameters(), lr=CFG['LEARNING_RATE'])\n",
    "scheduler_MLM = torch.optim.lr_scheduler.LambdaLR(optimizer = optimizer_MLM, lr_lambda= lambda epoch : 0.95**(epoch))\n",
    "optimizer_HLM = torch.optim.Adam(model_HLM.parameters(), lr=CFG['LEARNING_RATE'])\n",
    "scheduler_HLM = torch.optim.lr_scheduler.LambdaLR(optimizer = optimizer_HLM, lr_lambda= lambda epoch : 0.95**(epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, valid_loader, model, criterion, optimizer, scheduler,  epochs, label_scaling:Union[None, List] = None):\n",
    "\n",
    "    earlyStop = EarlyStopping(patience= 8, min_delta=-10, save_best=True)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0\n",
    "        for inputs_tab, inputs_fps, targets in train_loader:\n",
    "            optimizer.zero_grad() # Zero your gradients for every batch!\n",
    "            \n",
    "            output = model(inputs_tab.to(\"cuda\"), inputs_fps.to(\"cuda\"))\n",
    "            loss = criterion(output, targets.to(\"cuda\"))\n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step() # Adjust learning weights\n",
    "\n",
    "            if label_scaling is None:\n",
    "                running_loss += loss.item()\n",
    "\n",
    "            else:\n",
    "                metric_loss = label_scaling[1](label_scaling[0].inverse_transform(output.tolist()), targets.tolist())\n",
    "                running_loss += metric_loss\n",
    "            \n",
    "        \n",
    "        if epoch % 100 == 0:\n",
    "            model.eval()\n",
    "            valid_loss = 0\n",
    "            with torch.no_grad():\n",
    "                for inputs_tab, inputs_fps, targets in valid_loader:\n",
    "                    output = model(inputs_tab.to(\"cuda\"), inputs_fps.to(\"cuda\"))\n",
    "                    loss = criterion(output, targets.to(\"cuda\"))\n",
    "\n",
    "\n",
    "                    if label_scaling is None:\n",
    "                        valid_loss += loss.item()\n",
    "                    else:\n",
    "                        valid_metric_loss = label_scaling[1](label_scaling[0].inverse_transform(output.tolist()), targets.tolist())\n",
    "                        valid_loss += valid_metric_loss\n",
    "                    \n",
    "            print(f\"Epoch: {epoch:4d}/{epochs} with lr {scheduler.get_last_lr()[0]:.9f}, Train Loss: {np.sqrt(running_loss/len(train_loader))}, Valid Loss: {np.sqrt(valid_loss/len(valid_loader))}\")\n",
    "            \n",
    "            if earlyStop(valid_loss, verbose=True):\n",
    "                break\n",
    "\n",
    "            scheduler.step()    \n",
    "        \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:    0/8000 with lr 0.001000000, Train Loss: 49.79068179853604, Valid Loss: 50.212444992864654\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "save() got an unexpected keyword argument 'map_location'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model_MLM \u001b[39m=\u001b[39m train(train_MLM_loader, valid_MLM_loader, model_MLM, criterion, optimizer_MLM, scheduler_MLM, epochs\u001b[39m=\u001b[39;49mCFG[\u001b[39m\"\u001b[39;49m\u001b[39mEPOCHS\u001b[39;49m\u001b[39m\"\u001b[39;49m], label_scaling\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m      2\u001b[0m model_HLM \u001b[39m=\u001b[39m train(train_HLM_loader, valid_HLM_loader, model_HLM, criterion, optimizer_HLM, scheduler_HLM, epochs\u001b[39m=\u001b[39mCFG[\u001b[39m\"\u001b[39m\u001b[39mEPOCHS\u001b[39m\u001b[39m\"\u001b[39m], label_scaling\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[39], line 42\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(train_loader, valid_loader, model, criterion, optimizer, scheduler, epochs, label_scaling)\u001b[0m\n\u001b[1;32m     38\u001b[0m             valid_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m valid_metric_loss\n\u001b[1;32m     40\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch: \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m:\u001b[39;00m\u001b[39m4d\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mepochs\u001b[39m}\u001b[39;00m\u001b[39m with lr \u001b[39m\u001b[39m{\u001b[39;00mscheduler\u001b[39m.\u001b[39mget_last_lr()[\u001b[39m0\u001b[39m]\u001b[39m:\u001b[39;00m\u001b[39m.9f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, Train Loss: \u001b[39m\u001b[39m{\u001b[39;00mnp\u001b[39m.\u001b[39msqrt(running_loss\u001b[39m/\u001b[39m\u001b[39mlen\u001b[39m(train_loader))\u001b[39m}\u001b[39;00m\u001b[39m, Valid Loss: \u001b[39m\u001b[39m{\u001b[39;00mnp\u001b[39m.\u001b[39msqrt(valid_loss\u001b[39m/\u001b[39m\u001b[39mlen\u001b[39m(valid_loader))\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 42\u001b[0m \u001b[39mif\u001b[39;00m earlyStop(valid_loss, verbose\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m):\n\u001b[1;32m     43\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m     45\u001b[0m scheduler\u001b[39m.\u001b[39mstep()    \n",
      "Cell \u001b[0;32mIn[20], line 15\u001b[0m, in \u001b[0;36mEarlyStopping.__call__\u001b[0;34m(self, validation_loss, verbose)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcounter \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m  \u001b[39m# reset the counter if validation loss decreased at least by min_delta\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msave_best \u001b[39mis\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m---> 15\u001b[0m         torch\u001b[39m.\u001b[39;49msave(\u001b[39m\"\u001b[39;49m\u001b[39m../archive_model/saved_model.pt\u001b[39;49m\u001b[39m\"\u001b[39;49m, map_location \u001b[39m=\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mcuda:0\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     17\u001b[0m \u001b[39melif\u001b[39;00m ((validation_loss\u001b[39m+\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmin_delta) \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmin_validation_loss):\n\u001b[1;32m     18\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcounter \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m \u001b[39m# increase the counter if validation loss is not decreased by the min_delta\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: save() got an unexpected keyword argument 'map_location'"
     ]
    }
   ],
   "source": [
    "model_MLM = train(train_MLM_loader, valid_MLM_loader, model_MLM, criterion, optimizer_MLM, scheduler_MLM, epochs=CFG[\"EPOCHS\"], label_scaling=None)\n",
    "model_HLM = train(train_HLM_loader, valid_HLM_loader, model_HLM, criterion, optimizer_HLM, scheduler_HLM, epochs=CFG[\"EPOCHS\"], label_scaling=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(test_loader, model, label_scaler=None):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs_tab, inputs_fps in test_loader:\n",
    "            output = model(inputs_tab.to(\"cuda\"), inputs_fps.to(\"cuda\"))\n",
    "            if label_scaler is not None:\n",
    "                output = label_scaler.inverse_transform(output.cpu())\n",
    "            preds.extend(output.flatten().tolist())\n",
    "    \n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_MLM = inference(test_MLM_loader, model_MLM, label_scaler=label_scaler)\n",
    "predictions_HLM = inference(test_HLM_loader, model_HLM, label_scaler=label_scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('../input/sample_submission.csv')\n",
    "submission['MLM'] = predictions_MLM\n",
    "submission['HLM'] = predictions_HLM\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('../output/submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "smils",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
