{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from rdkit import DataStructs\n",
    "from rdkit import Chem, DataStructs\n",
    "from rdkit.Chem import AllChem, Descriptors\n",
    "\n",
    "from typing import List, Union\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def morgan_binary_features_generator(mol: Union[str, Chem.Mol], plot_img = False,\n",
    "                                     radius: int = 6,\n",
    "                                     num_bits: int = 4096) -> np.ndarray:\n",
    "    \n",
    "    mol = Chem.MolFromSmiles(mol) if type(mol) == str else mol\n",
    "    if plot_img:\n",
    "        display(mol)\n",
    "    \n",
    "    features_vec = AllChem.GetHashedMorganFingerprint(mol, radius, nBits=num_bits)\n",
    "    features = np.zeros((1,), dtype=np.int8)\n",
    "    DataStructs.ConvertToNumpyArray(features_vec, features)\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMolDescriptors(mol: Union[str, Chem.Mol], missingVal=None):\n",
    "    ''' calculate the full list of descriptors for a molecule\n",
    "\n",
    "        missingVal is used if the descriptor cannot be calculated\n",
    "    '''\n",
    "    mol = Chem.MolFromSmiles(mol) if type(mol) == str else mol\n",
    "    res = {}\n",
    "    for nm,fn in Descriptors._descList:\n",
    "        # some of the descriptor fucntions can throw errors if they fail, catch those here:\n",
    "        try:\n",
    "            val = fn(mol)\n",
    "        except:\n",
    "            # print the error message:\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            # and set the descriptor value to whatever missingVal is\n",
    "            val = missingVal\n",
    "        res[nm] = val\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, min_delta=-1):\n",
    "        self.patience = patience  # number of times to allow for no improvement before stopping the execution\n",
    "        self.min_delta = min_delta  # the minimum change to be counted as improvement\n",
    "        self.counter = 0  # count the number of times the validation accuracy not improving\n",
    "        self.min_validation_loss = np.inf\n",
    "\n",
    "    # return True when encountering _patience_ times decrease in validation loss \n",
    "    def __call__(self, validation_loss, verbose=False):\n",
    "        if ((validation_loss+self.min_delta) < self.min_validation_loss):\n",
    "            self.min_validation_loss = validation_loss\n",
    "            self.counter = 0  # reset the counter if validation loss decreased at least by min_delta\n",
    "        elif ((validation_loss+self.min_delta) > self.min_validation_loss):\n",
    "            self.counter += 1 # increase the counter if validation loss is not decreased by the min_delta\n",
    "            if verbose:\n",
    "                print(f\"  >> now{validation_loss:.3f} > best{self.min_validation_loss:.3f}\")\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"../input/train.csv\")\n",
    "test_df = pd.read_csv(\"../input/test.csv\")\n",
    "\n",
    "train_df[\"AlogP\"].fillna(value=train_df[\"AlogP\"].mean(), inplace=True)\n",
    "test_df[\"AlogP\"].fillna(value=test_df[\"AlogP\"].mean(), inplace=True)\n",
    "train_df.dropna(axis=0, inplace=True)\n",
    "train_df.drop_duplicates([\"SMILES\"], inplace=True)\n",
    "\n",
    "train_fps = pd.DataFrame(train_df[\"SMILES\"].apply(morgan_binary_features_generator).tolist())\n",
    "test_fps = pd.DataFrame(test_df[\"SMILES\"].apply(morgan_binary_features_generator).tolist())\n",
    "\n",
    "train_fps.rename(columns=lambda x: \"FPS_\" + str(x), inplace=True)\n",
    "test_fps.rename(columns=lambda x: \"FPS_\" + str(x), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_descriptor = pd.DataFrame([getMolDescriptors(smile) for smile in train_df['SMILES']])\n",
    "test_descriptor =  pd.DataFrame([getMolDescriptors(smile) for smile in test_df['SMILES']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.concat([train_df, train_descriptor], axis=1)\n",
    "test_df = pd.concat([test_df, test_descriptor], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.drop(columns=['AlogP', 'MolWt', 'NumHAcceptors', 'NumHDonors', 'NumRotatableBonds', 'MolLogP'], inplace=True)\n",
    "test_df.drop(columns=['AlogP', 'MolWt', 'NumHAcceptors', 'NumHDonors', 'NumRotatableBonds', 'MolLogP'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.fillna(train_df.mean(numeric_only=True), inplace=True)\n",
    "test_df.fillna(test_df.mean(numeric_only=True), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, tab_df, fps_df,  target: str, tab_scaler, label_scaler=None, is_test=False):\n",
    "        self.tab_df = tab_df\n",
    "        self.fps_df = fps_df\n",
    "        self.target = target\n",
    "        self.is_test = is_test\n",
    "        self.tab_scaler = tab_scaler\n",
    "\n",
    "\n",
    "        if self.is_test:\n",
    "            self.drop_col = [\"id\", \"SMILES\"]\n",
    "            self.tab_features = self.tab_scaler[1].transform(self.tab_scaler[0].transform(self.tab_df.drop(columns = self.drop_col, axis=1)))\n",
    "            self.fps_features = self.fps_df.values\n",
    "\n",
    "        else:\n",
    "            self.drop_col = [\"id\", \"SMILES\", \"MLM\", \"HLM\"]\n",
    "            self.tab_features = self.tab_scaler[1].fit_transform(self.tab_scaler[0].fit_transform(self.tab_df.drop(columns = self.drop_col, axis=1)))\n",
    "            self.fps_features = self.fps_df.values\n",
    "\n",
    "            if label_scaler is None:\n",
    "                self.label = self.tab_df[target].values.reshape(-1, 1)\n",
    "            else:\n",
    "                self.label = label_scaler.fit_transform(self.tab_df[[target]])\n",
    "\n",
    "            self.range_class = self.tab_df[target].apply(lambda x : np.int8(min(x, 100)//10)) # 구간 균등화 startify를 위함\n",
    "            \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        tab_feautres = self.tab_features[index]\n",
    "        fps_feautres = self.fps_features[index]\n",
    "\n",
    "        if self.is_test:\n",
    "            return torch.tensor(tab_feautres).float(), torch.tensor(fps_feautres).float()\n",
    "        else:\n",
    "            label = self.label[index]\n",
    "            return torch.tensor(tab_feautres).float(), torch.tensor(fps_feautres).float(), torch.tensor(label).float()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tab_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tab_scaler = [VarianceThreshold(threshold=0), MinMaxScaler()]\n",
    "label_scaler = None\n",
    "\n",
    "train_MLM = CustomDataset(tab_df = train_df, fps_df = train_fps, target=\"MLM\", tab_scaler = tab_scaler,  label_scaler=label_scaler, is_test= False)\n",
    "test_MLM = CustomDataset(tab_df = test_df, fps_df = test_fps, target=\"MLM\", tab_scaler = tab_scaler, label_scaler=label_scaler, is_test= True)\n",
    "\n",
    "train_HLM = CustomDataset(tab_df = train_df, fps_df = train_fps, target=\"HLM\", tab_scaler = tab_scaler, label_scaler=label_scaler, is_test= False)\n",
    "test_HLM = CustomDataset(tab_df = test_df, fps_df = test_fps, target=\"HLM\", tab_scaler = tab_scaler, label_scaler=label_scaler, is_test= True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "209\n"
     ]
    }
   ],
   "source": [
    "input_size = train_MLM.tab_features.shape[1] + 32 \n",
    "#input_size = train_MLM.fps_features.shape[1]\n",
    "print(input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter\n",
    "CFG = {'BATCH_SIZE': 256,\n",
    "       'EPOCHS': 8000,\n",
    "       'INPUT_SIZE': input_size,\n",
    "       'HIDDEN_SIZE': 1024,\n",
    "       'OUTPUT_SIZE': 1,\n",
    "       'DROPOUT_RATE': 0.8,\n",
    "       'LEARNING_RATE': 0.001}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     1345\n",
       "9      410\n",
       "1      254\n",
       "8      244\n",
       "7      240\n",
       "2      219\n",
       "4      201\n",
       "6      200\n",
       "5      193\n",
       "3      189\n",
       "10       3\n",
       "Name: MLM, dtype: int64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_MLM.range_class.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train,valid split\n",
    "train_MLM_dataset, valid_MLM_dataset = train_test_split(train_MLM, test_size=0.2, random_state=42, stratify=train_MLM.range_class)\n",
    "train_HLM_dataset, valid_HLM_dataset = train_test_split(train_HLM, test_size=0.2, random_state=42, stratify=train_HLM.range_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_MLM_loader = DataLoader(dataset=train_MLM_dataset,\n",
    "                              batch_size=CFG['BATCH_SIZE'],\n",
    "                              shuffle=True)\n",
    "\n",
    "valid_MLM_loader = DataLoader(dataset=valid_MLM_dataset,\n",
    "                              batch_size=CFG['BATCH_SIZE'],\n",
    "                              shuffle=False)\n",
    "\n",
    "train_HLM_loader = DataLoader(dataset=train_HLM_dataset,\n",
    "                              batch_size=CFG['BATCH_SIZE'],\n",
    "                              shuffle=True)\n",
    "\n",
    "valid_HLM_loader = DataLoader(dataset=valid_HLM_dataset,\n",
    "                              batch_size=CFG['BATCH_SIZE'],\n",
    "                              shuffle=False)\n",
    "\n",
    "test_MLM_loader = DataLoader(dataset=test_MLM,\n",
    "                              batch_size=CFG['BATCH_SIZE'],\n",
    "                              shuffle=False)\n",
    "\n",
    "test_HLM_loader = DataLoader(dataset=test_HLM,\n",
    "                              batch_size=CFG['BATCH_SIZE'],\n",
    "                              shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FpsAutoEncoder(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(FpsAutoEncoder, self).__init__()\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_size, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, output_size)\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(output_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, input_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.encoder(x)\n",
    "        out = self.decoder(out)\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def get_codes(self, x):\n",
    "        return self.encoder(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder.0.weight: torch.Size([512, 4096])\n",
      "encoder.0.bias: torch.Size([512])\n",
      "encoder.1.weight: torch.Size([512])\n",
      "encoder.1.bias: torch.Size([512])\n",
      "encoder.4.weight: torch.Size([256, 512])\n",
      "encoder.4.bias: torch.Size([256])\n",
      "encoder.5.weight: torch.Size([256])\n",
      "encoder.5.bias: torch.Size([256])\n",
      "encoder.8.weight: torch.Size([32, 256])\n",
      "encoder.8.bias: torch.Size([32])\n",
      "decoder.0.weight: torch.Size([256, 32])\n",
      "decoder.0.bias: torch.Size([256])\n",
      "decoder.2.weight: torch.Size([512, 256])\n",
      "decoder.2.bias: torch.Size([512])\n",
      "decoder.4.weight: torch.Size([4096, 512])\n",
      "decoder.4.bias: torch.Size([4096])\n"
     ]
    }
   ],
   "source": [
    "model_fps_encoder = FpsAutoEncoder(4096, 32)\n",
    "model_fps_encoder.load_state_dict(torch.load(\"../archive_model/autoEncoder.pt\"))\n",
    "\n",
    "for name, param in model_fps_encoder.named_parameters():\n",
    "    print(f\"{name}: {param.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, dropout_rate, out_size, encoder):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.fps_encoder = encoder\n",
    "        \n",
    "        for p in self.fps_encoder.parameters():\n",
    "            p.requires_grad = False ## 모델 freeze\n",
    "        \n",
    "        # fc 레이어 3개와 출력 레이어\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size) \n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "        self.fc_out = nn.Linear(hidden_size +input_size, out_size)\n",
    "        \n",
    "        # 정규화\n",
    "        self.ln1 = nn.BatchNorm1d(hidden_size)\n",
    "        self.ln2 = nn.BatchNorm1d(hidden_size)\n",
    "\n",
    "        # 활성화 함수\n",
    "        self.activation = nn.LeakyReLU()\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "     \n",
    "    def forward(self, tab_x, fps_x):\n",
    "\n",
    "        enc = self.fps_encoder.get_codes(fps_x)\n",
    "\n",
    "        x = torch.cat([enc, tab_x], dim=1)        \n",
    "\n",
    "        out1 = self.fc1(x)\n",
    "        out1 = self.ln1(out1)\n",
    "        out1 = self.activation(out1)\n",
    "        out1 = self.dropout(out1)\n",
    "        \n",
    "        out2 = self.fc2(out1)\n",
    "        out2 = self.ln2(out2)\n",
    "        out2 = self.activation(out2)\n",
    "        out2 = self.dropout(out2)\n",
    "\n",
    "        out3 = torch.cat([x , out2], dim=1)\n",
    "        \n",
    "        out = self.fc_out(out3)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_MLM = Net(CFG['INPUT_SIZE'],CFG['HIDDEN_SIZE'],CFG['DROPOUT_RATE'],CFG['OUTPUT_SIZE'], model_fps_encoder).to(\"cuda\")\n",
    "model_HLM = Net(CFG['INPUT_SIZE'],CFG['HIDDEN_SIZE'],CFG['DROPOUT_RATE'],CFG['OUTPUT_SIZE'], model_fps_encoder).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fps_encoder): FpsAutoEncoder(\n",
      "    (encoder): Sequential(\n",
      "      (0): Linear(in_features=4096, out_features=512, bias=True)\n",
      "      (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "      (3): Dropout(p=0.2, inplace=False)\n",
      "      (4): Linear(in_features=512, out_features=256, bias=True)\n",
      "      (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (6): ReLU()\n",
      "      (7): Dropout(p=0.2, inplace=False)\n",
      "      (8): Linear(in_features=256, out_features=32, bias=True)\n",
      "    )\n",
      "    (decoder): Sequential(\n",
      "      (0): Linear(in_features=32, out_features=256, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=256, out_features=512, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear(in_features=512, out_features=4096, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (fc1): Linear(in_features=209, out_features=1024, bias=True)\n",
      "  (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (fc_out): Linear(in_features=1233, out_features=1, bias=True)\n",
      "  (ln1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (ln2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (activation): LeakyReLU(negative_slope=0.01)\n",
      "  (dropout): Dropout(p=0.8, inplace=False)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1269970"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(model_MLM)\n",
    "sum(p.numel() for p in model_MLM.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer_MLM = torch.optim.Adam(model_MLM.parameters(), lr=CFG['LEARNING_RATE'])\n",
    "scheduler_MLM = torch.optim.lr_scheduler.LambdaLR(optimizer = optimizer_MLM, lr_lambda= lambda epoch : 0.95**(epoch))\n",
    "optimizer_HLM = torch.optim.Adam(model_HLM.parameters(), lr=CFG['LEARNING_RATE'])\n",
    "scheduler_HLM = torch.optim.lr_scheduler.LambdaLR(optimizer = optimizer_HLM, lr_lambda= lambda epoch : 0.95**(epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, valid_loader, model, criterion, optimizer, scheduler,  epochs, label_scaling:Union[None, List] = None):\n",
    "\n",
    "    earlyStop = EarlyStopping(patience= 8, min_delta=-10)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0\n",
    "        for inputs_tab, inputs_fps, targets in train_loader:\n",
    "            optimizer.zero_grad() # Zero your gradients for every batch!\n",
    "            \n",
    "            output = model(inputs_tab.to(\"cuda\"), inputs_fps.to(\"cuda\"))\n",
    "            loss = criterion(output, targets.to(\"cuda\"))\n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step() # Adjust learning weights\n",
    "\n",
    "            if label_scaling is None:\n",
    "                running_loss += loss.item()\n",
    "\n",
    "            else:\n",
    "                metric_loss = label_scaling[1](label_scaling[0].inverse_transform(output.tolist()), targets.tolist())\n",
    "                running_loss += metric_loss\n",
    "            \n",
    "        \n",
    "        if epoch % 100 == 0:\n",
    "            model.eval()\n",
    "            valid_loss = 0\n",
    "            with torch.no_grad():\n",
    "                for inputs_tab, inputs_fps, targets in valid_loader:\n",
    "                    output = model(inputs_tab.to(\"cuda\"), inputs_fps.to(\"cuda\"))\n",
    "                    loss = criterion(output, targets.to(\"cuda\"))\n",
    "\n",
    "\n",
    "                    if label_scaling is None:\n",
    "                        valid_loss += loss.item()\n",
    "                    else:\n",
    "                        valid_metric_loss = label_scaling[1](label_scaling[0].inverse_transform(output.tolist()), targets.tolist())\n",
    "                        valid_loss += valid_metric_loss\n",
    "                    \n",
    "            print(f\"Epoch: {epoch:4d}/{epochs} with lr {scheduler.get_last_lr()[0]:.9f}, Train Loss: {np.sqrt(running_loss/len(train_loader))}, Valid Loss: {np.sqrt(valid_loss/len(valid_loader))}\")\n",
    "            \n",
    "            if earlyStop(valid_loss, verbose=True):\n",
    "                break\n",
    "\n",
    "            scheduler.step()    \n",
    "        \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:    0/8000 with lr 0.001000000, Train Loss: 49.96390409621241, Valid Loss: 50.416865745130366\n",
      "Epoch:  100/8000 with lr 0.000950000, Train Loss: 25.218523707550393, Valid Loss: 30.480642711040804\n",
      "Epoch:  200/8000 with lr 0.000902500, Train Loss: 21.909402039979714, Valid Loss: 30.6965254118977\n",
      "  >> now2826.830 > best2787.209\n",
      "Epoch:  300/8000 with lr 0.000857375, Train Loss: 20.426559612477458, Valid Loss: 30.826711022544195\n",
      "  >> now2850.858 > best2787.209\n",
      "Epoch:  400/8000 with lr 0.000814506, Train Loss: 18.445734766040804, Valid Loss: 31.212087027746225\n",
      "  >> now2922.583 > best2787.209\n",
      "Epoch:  500/8000 with lr 0.000773781, Train Loss: 17.1402408287778, Valid Loss: 30.975967443843157\n",
      "  >> now2878.532 > best2787.209\n",
      "Epoch:  600/8000 with lr 0.000735092, Train Loss: 16.572738429985385, Valid Loss: 31.356484916546602\n",
      "  >> now2949.687 > best2787.209\n",
      "Epoch:  700/8000 with lr 0.000698337, Train Loss: 15.79337207945729, Valid Loss: 31.304814556039474\n",
      "  >> now2939.974 > best2787.209\n",
      "Epoch:  800/8000 with lr 0.000663420, Train Loss: 15.477539166128025, Valid Loss: 31.568301472800275\n",
      "  >> now2989.673 > best2787.209\n",
      "Epoch:  900/8000 with lr 0.000630249, Train Loss: 14.491627921995011, Valid Loss: 31.306984827525078\n",
      "  >> now2940.382 > best2787.209\n",
      "Epoch:    0/8000 with lr 0.001000000, Train Loss: 62.415998059334896, Valid Loss: 63.29642950657841\n",
      "Epoch:  100/8000 with lr 0.000950000, Train Loss: 26.14164627667972, Valid Loss: 33.655488931545094\n",
      "Epoch:  200/8000 with lr 0.000902500, Train Loss: 23.09634491078077, Valid Loss: 33.82744117380425\n",
      "  >> now3432.887 > best3398.076\n",
      "Epoch:  300/8000 with lr 0.000857375, Train Loss: 21.203983915346658, Valid Loss: 34.46444565523892\n",
      "  >> now3563.394 > best3398.076\n",
      "Epoch:  400/8000 with lr 0.000814506, Train Loss: 19.47339648936091, Valid Loss: 34.85480135841223\n",
      "  >> now3644.572 > best3398.076\n",
      "Epoch:  500/8000 with lr 0.000773781, Train Loss: 18.056924761687807, Valid Loss: 35.07361584789674\n",
      "  >> now3690.476 > best3398.076\n",
      "Epoch:  600/8000 with lr 0.000735092, Train Loss: 17.34635401117228, Valid Loss: 34.81017335414049\n",
      "  >> now3635.245 > best3398.076\n",
      "Epoch:  700/8000 with lr 0.000698337, Train Loss: 16.758533472782776, Valid Loss: 35.667494529552044\n",
      "  >> now3816.510 > best3398.076\n",
      "Epoch:  800/8000 with lr 0.000663420, Train Loss: 15.999597414327129, Valid Loss: 35.376776516971844\n",
      "  >> now3754.549 > best3398.076\n",
      "Epoch:  900/8000 with lr 0.000630249, Train Loss: 15.18964610294421, Valid Loss: 35.59634949901815\n",
      "  >> now3801.300 > best3398.076\n"
     ]
    }
   ],
   "source": [
    "model_MLM = train(train_MLM_loader, valid_MLM_loader, model_MLM, criterion, optimizer_MLM, scheduler_MLM, epochs=CFG[\"EPOCHS\"], label_scaling=None)\n",
    "model_HLM = train(train_HLM_loader, valid_HLM_loader, model_HLM, criterion, optimizer_HLM, scheduler_HLM, epochs=CFG[\"EPOCHS\"], label_scaling=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(test_loader, model, label_scaler=None):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs_tab, inputs_fps in test_loader:\n",
    "            output = model(inputs_tab.to(\"cuda\"), inputs_fps.to(\"cuda\"))\n",
    "            if label_scaler is not None:\n",
    "                output = label_scaler.inverse_transform(output.cpu())\n",
    "            preds.extend(output.flatten().tolist())\n",
    "    \n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_MLM = inference(test_MLM_loader, model_MLM, label_scaler=label_scaler)\n",
    "predictions_HLM = inference(test_HLM_loader, model_HLM, label_scaler=label_scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>MLM</th>\n",
       "      <th>HLM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TEST_000</td>\n",
       "      <td>0.201058</td>\n",
       "      <td>9.581748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TEST_001</td>\n",
       "      <td>57.406219</td>\n",
       "      <td>80.907387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TEST_002</td>\n",
       "      <td>56.564281</td>\n",
       "      <td>73.834068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TEST_003</td>\n",
       "      <td>50.966148</td>\n",
       "      <td>67.596542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TEST_004</td>\n",
       "      <td>57.561031</td>\n",
       "      <td>63.305035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>TEST_478</td>\n",
       "      <td>28.630735</td>\n",
       "      <td>8.052299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479</th>\n",
       "      <td>TEST_479</td>\n",
       "      <td>87.059135</td>\n",
       "      <td>84.735931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>480</th>\n",
       "      <td>TEST_480</td>\n",
       "      <td>7.877865</td>\n",
       "      <td>26.742342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481</th>\n",
       "      <td>TEST_481</td>\n",
       "      <td>53.881615</td>\n",
       "      <td>59.072704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482</th>\n",
       "      <td>TEST_482</td>\n",
       "      <td>8.926653</td>\n",
       "      <td>71.518730</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>483 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id        MLM        HLM\n",
       "0    TEST_000   0.201058   9.581748\n",
       "1    TEST_001  57.406219  80.907387\n",
       "2    TEST_002  56.564281  73.834068\n",
       "3    TEST_003  50.966148  67.596542\n",
       "4    TEST_004  57.561031  63.305035\n",
       "..        ...        ...        ...\n",
       "478  TEST_478  28.630735   8.052299\n",
       "479  TEST_479  87.059135  84.735931\n",
       "480  TEST_480   7.877865  26.742342\n",
       "481  TEST_481  53.881615  59.072704\n",
       "482  TEST_482   8.926653  71.518730\n",
       "\n",
       "[483 rows x 3 columns]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = pd.read_csv('../input/sample_submission.csv')\n",
    "submission['MLM'] = predictions_MLM\n",
    "submission['HLM'] = predictions_HLM\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MLM</th>\n",
       "      <th>HLM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>483.000000</td>\n",
       "      <td>483.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>32.541354</td>\n",
       "      <td>45.860658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>26.377052</td>\n",
       "      <td>25.511470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-2.368363</td>\n",
       "      <td>-2.204836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>8.044451</td>\n",
       "      <td>25.029202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>28.395479</td>\n",
       "      <td>43.250042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>53.696426</td>\n",
       "      <td>68.332569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>104.684044</td>\n",
       "      <td>113.410904</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              MLM         HLM\n",
       "count  483.000000  483.000000\n",
       "mean    32.541354   45.860658\n",
       "std     26.377052   25.511470\n",
       "min     -2.368363   -2.204836\n",
       "25%      8.044451   25.029202\n",
       "50%     28.395479   43.250042\n",
       "75%     53.696426   68.332569\n",
       "max    104.684044  113.410904"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('../output/submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "smils",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
