{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from rdkit import DataStructs\n",
    "from rdkit import Chem, DataStructs\n",
    "from rdkit.Chem import AllChem, Descriptors\n",
    "\n",
    "from typing import List, Union\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, min_delta : float=-1.0):\n",
    "        self.patience = patience  # number of times to allow for no improvement before stopping the execution\n",
    "        self.min_delta = min_delta  # the minimum change to be counted as improvement\n",
    "        self.counter = 0  # count the number of times the validation accuracy not improving\n",
    "        self.min_validation_loss = np.inf\n",
    "\n",
    "    # return True when encountering _patience_ times decrease in validation loss \n",
    "    def __call__(self, validation_loss, verbose=False):\n",
    "        if ((validation_loss+self.min_delta) < self.min_validation_loss):\n",
    "            self.min_validation_loss = validation_loss\n",
    "            self.counter = 0  # reset the counter if validation loss decreased at least by min_delta\n",
    "        elif ((validation_loss+self.min_delta) > self.min_validation_loss):\n",
    "            self.counter += 1 # increase the counter if validation loss is not decreased by the min_delta\n",
    "            if verbose:\n",
    "                print(f\"  >> now{validation_loss:.3f} > best{self.min_validation_loss:.3f}\")\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"../input/train_features.csv\")\n",
    "test_df = pd.read_csv(\"../input/test_features.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, tab_df, target: str, is_test=False):\n",
    "        self.tab_df = tab_df\n",
    "        self.target = target\n",
    "        self.is_test = is_test\n",
    "\n",
    "        if self.is_test:\n",
    "            self.drop_col = [\"id\", \"SMILES\"]\n",
    "            self.tab_features = self.tab_df.drop(columns = self.drop_col, axis=1).values\n",
    "\n",
    "        else:\n",
    "            self.label = self.tab_df[target].values.reshape(-1, 1)\n",
    "            self.drop_col = [\"id\", \"SMILES\", \"MLM\", \"HLM\"]\n",
    "            self.tab_features = self.tab_df.drop(columns = self.drop_col, axis=1).values\n",
    "            #self.fps_features = self.fps_scaler.fit_transform(self.fps_df)\n",
    "\n",
    "            self.range_class = self.tab_df[target].apply(lambda x : np.int8(min(x, 100)//10)) # 구간 균등화 startify를 위함\n",
    "            \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        feature  = self.tab_features[index]\n",
    "        \n",
    "\n",
    "        if self.is_test:\n",
    "            return torch.tensor(feature).float()\n",
    "        else:\n",
    "            label = self.label[index]\n",
    "            return torch.tensor(feature).float(), torch.tensor(label).float()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tab_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_MLM = CustomDataset(tab_df = train_df, target=\"MLM\", is_test= False)\n",
    "test_MLM = CustomDataset(tab_df = test_df,  target=\"MLM\", is_test= True)\n",
    "\n",
    "train_HLM = CustomDataset(tab_df = train_df,  target=\"HLM\",  is_test= False)\n",
    "test_HLM = CustomDataset(tab_df = test_df,  target=\"HLM\", is_test= True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "233\n"
     ]
    }
   ],
   "source": [
    "input_size = train_MLM.tab_features.shape[1] \n",
    "#input_size = train_MLM.fps_features.shape[1]\n",
    "print(input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter\n",
    "CFG = {'BATCH_SIZE': 256,\n",
    "       'EPOCHS': 8000,\n",
    "       'INPUT_SIZE': input_size,\n",
    "       'HIDDEN_SIZE': 1024,\n",
    "       'OUTPUT_SIZE': 1,\n",
    "       'DROPOUT_RATE': 0.8,\n",
    "       'LEARNING_RATE': 0.001}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1333\n",
       "9     407\n",
       "1     252\n",
       "8     239\n",
       "7     237\n",
       "2     219\n",
       "4     198\n",
       "6     196\n",
       "5     191\n",
       "3     189\n",
       "Name: MLM, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_MLM.range_class.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train,valid split\n",
    "train_MLM_dataset, valid_MLM_dataset = train_test_split(train_MLM, test_size=0.2, random_state=42, stratify=train_MLM.range_class)\n",
    "train_HLM_dataset, valid_HLM_dataset = train_test_split(train_HLM, test_size=0.2, random_state=42, stratify=train_HLM.range_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_MLM_loader = DataLoader(dataset=train_MLM_dataset,\n",
    "                              batch_size=CFG['BATCH_SIZE'],\n",
    "                              shuffle=True)\n",
    "\n",
    "valid_MLM_loader = DataLoader(dataset=valid_MLM_dataset,\n",
    "                              batch_size=CFG['BATCH_SIZE'],\n",
    "                              shuffle=False)\n",
    "\n",
    "train_HLM_loader = DataLoader(dataset=train_HLM_dataset,\n",
    "                              batch_size=CFG['BATCH_SIZE'],\n",
    "                              shuffle=True)\n",
    "\n",
    "valid_HLM_loader = DataLoader(dataset=valid_HLM_dataset,\n",
    "                              batch_size=CFG['BATCH_SIZE'],\n",
    "                              shuffle=False)\n",
    "\n",
    "test_MLM_loader = DataLoader(dataset=test_MLM,\n",
    "                              batch_size=CFG['BATCH_SIZE'],\n",
    "                              shuffle=False)\n",
    "\n",
    "test_HLM_loader = DataLoader(dataset=test_HLM,\n",
    "                              batch_size=CFG['BATCH_SIZE'],\n",
    "                              shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, dropout_rate, out_size):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # fc 레이어 3개와 출력 레이어\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size) \n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc4 = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "        self.fc_out = nn.Linear(hidden_size, out_size)\n",
    "        \n",
    "        # 정규화\n",
    "        self.ln1 = nn.BatchNorm1d(hidden_size)\n",
    "        self.ln2 = nn.BatchNorm1d(hidden_size)\n",
    "        self.ln3 = nn.BatchNorm1d(hidden_size)        \n",
    "        self.ln4 = nn.BatchNorm1d(hidden_size)        \n",
    "\n",
    "        # 활성화 함수\n",
    "        self.activation = nn.LeakyReLU()\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "     \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.ln1(out)\n",
    "        out = self.activation(out)\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        out = self.fc2(out)\n",
    "        out = self.ln2(out)\n",
    "        out = self.activation(out)\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        out = self.fc3(out)\n",
    "        out = self.ln3(out)\n",
    "        out = self.activation(out)\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        out = self.fc4(out)\n",
    "        out = self.ln4(out)\n",
    "        out = self.activation(out)\n",
    "        out = self.dropout(out)\n",
    "\n",
    "\n",
    "        out = self.fc_out(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_MLM = Net(CFG['INPUT_SIZE'],CFG['HIDDEN_SIZE'],CFG['DROPOUT_RATE'],CFG['OUTPUT_SIZE']).to(\"cuda\")\n",
    "model_HLM = Net(CFG['INPUT_SIZE'],CFG['HIDDEN_SIZE'],CFG['DROPOUT_RATE'],CFG['OUTPUT_SIZE']).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=233, out_features=1024, bias=True)\n",
      "  (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (fc3): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (fc4): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (fc_out): Linear(in_features=1024, out_features=1, bias=True)\n",
      "  (ln1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (ln2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (ln3): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (ln4): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (activation): LeakyReLU(negative_slope=0.01)\n",
      "  (dropout): Dropout(p=0.8, inplace=False)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3397633"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(model_MLM)\n",
    "sum(p.numel() for p in model_MLM.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer_MLM = torch.optim.Adam(model_MLM.parameters(), lr=CFG['LEARNING_RATE'])\n",
    "scheduler_MLM = torch.optim.lr_scheduler.LambdaLR(optimizer = optimizer_MLM, lr_lambda= lambda epoch : 0.95**(epoch))\n",
    "optimizer_HLM = torch.optim.Adam(model_HLM.parameters(), lr=CFG['LEARNING_RATE'])\n",
    "scheduler_HLM = torch.optim.lr_scheduler.LambdaLR(optimizer = optimizer_HLM, lr_lambda= lambda epoch : 0.95**(epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, valid_loader, model, criterion, optimizer, scheduler,  epochs):\n",
    "\n",
    "    earlyStop = EarlyStopping(patience= 8, min_delta=-10)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0\n",
    "        for inputs, targets in train_loader:\n",
    "            optimizer.zero_grad() # Zero your gradients for every batch!\n",
    "            \n",
    "            output = model(inputs.to(\"cuda\"))\n",
    "            loss = criterion(output, targets.to(\"cuda\"))\n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step() # Adjust learning weights\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        if epoch % 20 == 0:\n",
    "            model.eval()\n",
    "            valid_loss = 0\n",
    "            with torch.no_grad():\n",
    "                for inputs, targets in valid_loader:\n",
    "                    output = model(inputs.to(\"cuda\"))\n",
    "                    loss = criterion(output, targets.to(\"cuda\"))\n",
    "                    valid_loss += loss.item()\n",
    "                 \n",
    "                    \n",
    "            print(f\"Epoch: {epoch:4d}/{epochs} with lr {scheduler.get_last_lr()[0]:.9f}, Train Loss: {np.sqrt(running_loss/len(train_loader))}, Valid Loss: {np.sqrt(valid_loss/len(valid_loader))}\")\n",
    "            \n",
    "            if earlyStop(valid_loss, verbose=True):\n",
    "                break\n",
    "\n",
    "            scheduler.step()    \n",
    "        \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:    0/8000 with lr 0.001000000, Train Loss: 50.456423082960455, Valid Loss: 51.28092943734867\n",
      "Epoch:   20/8000 with lr 0.000950000, Train Loss: 31.892685932220193, Valid Loss: 32.44545484156914\n",
      "Epoch:   40/8000 with lr 0.000902500, Train Loss: 30.10859574860259, Valid Loss: 32.205474476601346\n",
      "Epoch:   60/8000 with lr 0.000857375, Train Loss: 29.389683395729314, Valid Loss: 31.952241863305083\n",
      "Epoch:   80/8000 with lr 0.000814506, Train Loss: 28.57994565175083, Valid Loss: 32.48521486107661\n",
      "  >> now3165.868 > best3062.837\n",
      "Epoch:  100/8000 with lr 0.000773781, Train Loss: 27.348483762322388, Valid Loss: 33.29391875465648\n",
      "  >> now3325.455 > best3062.837\n",
      "Epoch:  120/8000 with lr 0.000735092, Train Loss: 26.72987281358306, Valid Loss: 32.94741021698838\n",
      "  >> now3256.596 > best3062.837\n",
      "Epoch:  140/8000 with lr 0.000698337, Train Loss: 26.118323562740066, Valid Loss: 35.188130221191656\n",
      "  >> now3714.614 > best3062.837\n",
      "Epoch:  160/8000 with lr 0.000663420, Train Loss: 25.217308602282593, Valid Loss: 34.24447995738997\n",
      "  >> now3518.053 > best3062.837\n",
      "Epoch:  180/8000 with lr 0.000630249, Train Loss: 24.549499106563612, Valid Loss: 34.08899306582973\n",
      "  >> now3486.178 > best3062.837\n",
      "Epoch:  200/8000 with lr 0.000598737, Train Loss: 24.17082231879873, Valid Loss: 34.43790172935676\n",
      "  >> now3557.907 > best3062.837\n",
      "Epoch:  220/8000 with lr 0.000568800, Train Loss: 22.952388230118597, Valid Loss: 34.846441646783795\n",
      "  >> now3642.823 > best3062.837\n",
      "Epoch:    0/8000 with lr 0.001000000, Train Loss: 62.19506470949886, Valid Loss: 63.24940749784242\n",
      "Epoch:   20/8000 with lr 0.000950000, Train Loss: 33.28575930425709, Valid Loss: 35.02277381750903\n",
      "Epoch:   40/8000 with lr 0.000902500, Train Loss: 31.409699094726072, Valid Loss: 33.66972875990576\n",
      "Epoch:   60/8000 with lr 0.000857375, Train Loss: 30.839555811994245, Valid Loss: 34.10215581528534\n",
      "  >> now3488.871 > best3400.952\n",
      "Epoch:   80/8000 with lr 0.000814506, Train Loss: 29.489568385357686, Valid Loss: 34.810912684488514\n",
      "  >> now3635.399 > best3400.952\n",
      "Epoch:  100/8000 with lr 0.000773781, Train Loss: 28.790949310625923, Valid Loss: 34.26758252217484\n",
      "  >> now3522.802 > best3400.952\n",
      "Epoch:  120/8000 with lr 0.000735092, Train Loss: 28.286594838049353, Valid Loss: 35.77989216855956\n",
      "  >> now3840.602 > best3400.952\n",
      "Epoch:  140/8000 with lr 0.000698337, Train Loss: 27.3859393477984, Valid Loss: 35.05719668467547\n",
      "  >> now3687.021 > best3400.952\n",
      "Epoch:  160/8000 with lr 0.000663420, Train Loss: 26.58532368791594, Valid Loss: 35.14254651908928\n",
      "  >> now3704.996 > best3400.952\n",
      "Epoch:  180/8000 with lr 0.000630249, Train Loss: 26.78026150616865, Valid Loss: 35.23805644108074\n",
      "  >> now3725.162 > best3400.952\n",
      "Epoch:  200/8000 with lr 0.000598737, Train Loss: 25.886150974566576, Valid Loss: 35.102077475542075\n",
      "  >> now3696.468 > best3400.952\n"
     ]
    }
   ],
   "source": [
    "model_MLM = train(train_MLM_loader, valid_MLM_loader, model_MLM, criterion, optimizer_MLM, scheduler_MLM, epochs=CFG[\"EPOCHS\"])\n",
    "model_HLM = train(train_HLM_loader, valid_HLM_loader, model_HLM, criterion, optimizer_HLM, scheduler_HLM, epochs=CFG[\"EPOCHS\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(test_loader, model, label_scaler=None):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx, inputs in enumerate(test_loader):\n",
    "            print(f\"{idx:3d}th: \", end=\" \")\n",
    "            for d in inputs:\n",
    "                std = np.std(d.detach().numpy())\n",
    "                print(f\"{std} \", end=\" \")\n",
    "            print()\n",
    "            output = model(inputs.to(\"cuda\"))\n",
    "            if label_scaler is not None:\n",
    "                output = label_scaler.inverse_transform(output.cpu())\n",
    "            preds.extend(output.flatten().tolist())\n",
    "    \n",
    "    return preds\n",
    "\n",
    "predictions_MLM = inference(test_MLM_loader, model_MLM, label_scaler=None)\n",
    "predictions_HLM = inference(test_HLM_loader, model_HLM, label_scaler=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('../input/sample_submission.csv')\n",
    "submission['MLM'] = predictions_MLM\n",
    "submission['HLM'] = predictions_HLM\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('../output/submission.csv', index=False)\n",
    "submission.describe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "smils",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
