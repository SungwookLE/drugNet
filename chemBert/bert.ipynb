{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ChemBERT를 backbone으로 사용 https://huggingface.co/models?sort=downloads&search=chemBERT  \n",
    "아이디어: GNN대신 트랜스포머계열을 활용해서 graph representation을 획득하는 (매우단순한) 모델  \n",
    "https://dacon.io/competitions/official/236127/codeshare/8812?page=1&dtype=recent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-25 00:09:56.861085: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-25 00:09:58.282402: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "No normalization for AvgIpc. Feature removed!\n",
      "No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    }
   ],
   "source": [
    "# molecule predictor \n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "import chem # 직접 작성한 chem.py 파일 추가함\n",
    "\n",
    "\n",
    "class ChemBERT(nn.Module):\n",
    "    def __init__(self, BERT_out_dim, projection_dim, out_dim) -> None:\n",
    "        super(ChemBERT, self).__init__()\n",
    "        self.ChemBERT_encoder = AutoModelForSequenceClassification.from_pretrained(\n",
    "            chem.chosen, num_labels=BERT_out_dim, problem_type=\"multi_label_classification\"\n",
    "        )\n",
    "        \n",
    "        # (classifier): RobertaClassificationHead(\n",
    "        #                 (dense): Linear(in_features=384, out_features=384, bias=True)\n",
    "        #                 (dropout): Dropout(p=0.144, inplace=False)\n",
    "        #                 (out_proj): Linear(in_features=384, out_features=BERT_out_dim, bias=True)\n",
    "        #                 )\n",
    "        \n",
    "        self.projection = nn.Linear(in_features=projection_dim, out_features=projection_dim)\n",
    "        self.ln = nn.LayerNorm(normalized_shape=projection_dim)\n",
    "        self.out = nn.Linear(in_features=projection_dim, out_features=out_dim)\n",
    "\n",
    "        self.act = nn.GELU()\n",
    "        self.drop = nn.Dropout(0.144)\n",
    "        \n",
    "    def forward(self, batch):\n",
    "        enc_out = self.ChemBERT_encoder(batch.input_ids).logits\n",
    "\n",
    "        h = torch.concat([enc_out, batch.mol_f], dim=1)\n",
    "        h = self.projection(h)\n",
    "        h = self.ln(h)\n",
    "        h = self.act(h)\n",
    "        h = self.drop(h)\n",
    "        h = self.out(h)\n",
    "        \n",
    "        return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Dataset(PyG)과 Pytorch_lighting을 활용한 Dataloader\n",
    "\n",
    "`chem` 에서 생성된 atomic & molecular feature를 dataset 형태로 만드는 코드\n",
    "\n",
    "- dataset in PyG\n",
    "    - using `torch_geometric`\n",
    "    - `batch.x (atomic feature) batch.edge_index(molecular bonds)`를 통해서 GCN forward에 제공하면 작동\n",
    "    - e.g. `h1 = self.conv1(g.x, g.edge_index)`\n",
    "\n",
    "- pl.LightningDataModule\n",
    "    - Dataloader를 만드는 클래스\n",
    "    - KFold를 상정하고 코드를 작성, torch-lightning 을 사용해서 더 간단하게 사용할 수 있음\n",
    "    - 학습데이터에 존재하는 LogP 결측치는 rdkit 패키지의 LogP 값으로 대체함\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Optional, Union\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "import torch \n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.data.dataset import IndexType\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from chem import Chemical_feature_generator\n",
    "from rdkit.Chem import PandasTools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_label = [ 'MolWt', 'HeavyAtomMolWt',\n",
    "                    'NumValenceElectrons', 'FractionCSP3', 'HeavyAtomCount', 'NHOHCount',\n",
    "                    'NOCount', 'NumAliphaticCarbocycles', 'NumAliphaticHeterocycles',\n",
    "                    'NumAliphaticRings', 'NumAromaticCarbocycles',\n",
    "                    'NumAromaticHeterocycles', 'NumAromaticRings', 'NumHAcceptors',\n",
    "                    'NumHDonors', 'NumHeteroatoms', 'NumRotatableBonds', 'RingCount',\n",
    "                    'MolMR', 'CalcNumBridgeheadAtom', 'ExactMolWt', \n",
    "                    'NumSaturatedCarbocycles', 'NumSaturatedHeterocycles',\n",
    "                    'NumSaturatedRings', 'MolLogP', 'CalcNumAmideBonds',\n",
    "                    'CalcNumSpiroAtoms',  \n",
    "                    'num_ammonium_groups',  'num_alkoxy_groups'] # 29 \n",
    "\n",
    "\n",
    "# given features가 이 모델에서 사용하는 tabular_features 임\n",
    "given_features = ['AlogP','Molecular_Weight','Num_H_Acceptors','Num_H_Donors','Num_RotatableBonds','LogD','Molecular_PolarSurfaceArea'] # 7 \n",
    "generator = Chemical_feature_generator()\n",
    "\n",
    "\n",
    "class Chemical_dataset(Dataset):\n",
    "    def __init__(self, data_frame: pd.DataFrame, fps, mol_f, transform = None, is_train = True):\n",
    "        super().__init__()\n",
    "        self.df = data_frame\n",
    "        self.fps = fps\n",
    "        self.mol_f = mol_f\n",
    "        self.transform = transform\n",
    "\n",
    "        self.is_train = is_train\n",
    "\n",
    "    def __getitem__(self, idx: IndexType | int):\n",
    "        return self.get_chem_prop(idx)\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return self.df.shape[0]\n",
    "    \n",
    "    def get_chem_prop(self, idx):\n",
    "\n",
    "        sample = self.df.iloc[idx]\n",
    "        fingerprint = self.fps[idx]\n",
    "        molecular_feature = self.mol_f[idx]\n",
    "        smiles = sample[\"SMILES\"]\n",
    "\n",
    "        edge_index, edge_attr = generator.get_adj_matrix(smiles = smiles)\n",
    "        atomic_feature = generator.generate_mol_atomic_features(smiles=smiles)\n",
    "        input_ids = generator.encoder_smiles(smiles) # 384\n",
    "        # ChemBERTa = ChemBERTa.detach()\n",
    "        # molecular_feature = sample[feature_label] # if we use VarianceThreshold, then block this code\n",
    "\n",
    "        if self.is_train:\n",
    "            MLM = sample[\"MLM\"]\n",
    "            HLM = sample[\"HLM\"]\n",
    "\n",
    "        else:\n",
    "            MLM = -99.\n",
    "            HLM = -99.\n",
    "\n",
    "        atomic_feature = torch.tensor(atomic_feature, dtype=torch.float)\n",
    "        molecular_feature = torch.tensor(molecular_feature, dtype = torch.float).view(1,-1)\n",
    "        fingerprint = torch.tensor(fingerprint, dtype=torch.float).view(1,-1)\n",
    "        MLM = torch.tensor(MLM, dtype=torch.float).view(1,-1)\n",
    "        HLM = torch.tensor(HLM, dtype=torch.float).view(1,-1)\n",
    "        y = torch.concat([MLM, HLM], dim = 1)\n",
    "\n",
    "        return Data(x=atomic_feature, mol_f = molecular_feature, fp=fingerprint,\n",
    "         edge_index = edge_index, edge_attr = edge_attr, input_ids = input_ids, \n",
    "         y = y, MLM = MLM, HLM = HLM)\n",
    "\n",
    "class KFold_pl_DataModule(pl.LightningDataModule):\n",
    "    def __init__(self,\n",
    "            train_df : str = \"../input/new_train.csv\",\n",
    "            k_idx : int = 1, # fold index\n",
    "            num_split : int = 5, # fold number, if k = 1 then return the whole data\n",
    "            split_seed: int = 41,\n",
    "            batch_size: int =1 ,\n",
    "            num_workers:int =0,\n",
    "            pin_memory:bool=False,\n",
    "            persistent_workers: bool = True,\n",
    "            train_transform = None,\n",
    "            val_transform = None) -> None:\n",
    "\n",
    "        super().__init__()\n",
    "        persistent_workers = True if num_workers > 0 else False\n",
    "        self.save_hyperparameters(logger=False)\n",
    "\n",
    "        self.train_data = None\n",
    "        self.val_data = None\n",
    "        self.num_cls = 0\n",
    "\n",
    "        self.setup()\n",
    "\n",
    "    def setup(self, stage = None) -> None:\n",
    "        if not self.train_data and not self.val_data:\n",
    "            df = pd.read_csv(self.hparams.train_df, index_col = 0)\n",
    "\n",
    "            mask = df['AlogP'] != df['AlogP']\n",
    "            df.loc[mask, 'AlogP'] = df.loc[mask, 'MolLogP']\n",
    "\n",
    "            # if we use rdkit fingerprint generators \n",
    "            # PandasTools.AddMoleculeColumnToFrame(df,'SMILES','Molecule')\n",
    "            # df[\"FPs\"] = df.Molecule.apply(generator.get_molecule_fingerprints)\n",
    "            # train_fps = np.stack(df[\"FPs\"])\n",
    "            mol2vec = []\n",
    "\n",
    "            for smiles in df.SMILES:\n",
    "                vec = generator.get_mol_feature_from_deepchem(smiles = smiles)\n",
    "                mol2vec.append(vec)\n",
    "\n",
    "            mol2vec = np.concatenate(mol2vec, axis=0)\n",
    "\n",
    "            scaler = preprocessing.StandardScaler()\n",
    "            craft_mol_f = df[given_features].to_numpy()\n",
    "            craft_mol_f = scaler.fit_transform(craft_mol_f)\n",
    "\n",
    "\n",
    "            kf = KFold(n_splits = self.hparams.num_split,\n",
    "                    shuffle=True,\n",
    "                    random_state = self.hparams.split_seed)\n",
    "            all_splits = [k for k in kf.split(df)]\n",
    "            train_idx, val_idx = all_splits[self.hparams.k_idx]\n",
    "            train_idx, val_idx = train_idx.tolist(), val_idx.tolist()\n",
    "\n",
    "            train_df = df.iloc[train_idx]\n",
    "            train_fp = mol2vec[train_idx]\n",
    "            train_mol_f = craft_mol_f[train_idx]\n",
    "\n",
    "            val_df = df.iloc[val_idx]\n",
    "            val_fp = mol2vec[val_idx]\n",
    "            val_mol_f = craft_mol_f[val_idx]\n",
    "\n",
    "            self.train_data = Chemical_dataset(data_frame=train_df, fps=train_fp, mol_f=train_mol_f, transform=None, is_train=True)\n",
    "            self.val_data = Chemical_dataset(data_frame=val_df, fps=val_fp, mol_f=val_mol_f, transform=None, is_train=True)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_data,\n",
    "                          batch_size=self.hparams.batch_size,\n",
    "                          shuffle=True,\n",
    "                          num_workers=self.hparams.num_workers,\n",
    "                          persistent_workers=self.hparams.persistent_workers,\n",
    "                          pin_memory=self.hparams.pin_memory,\n",
    "                          drop_last=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_data,\n",
    "                          batch_size=self.hparams.batch_size,\n",
    "                          shuffle=False,\n",
    "                          num_workers=self.hparams.num_workers,\n",
    "                          persistent_workers=self.hparams.persistent_workers,\n",
    "                          pin_memory=self.hparams.pin_memory)\n",
    "        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, min_delta=-1):\n",
    "        self.patience = patience  # number of times to allow for no improvement before stopping the execution\n",
    "        self.min_delta = min_delta  # the minimum change to be counted as improvement\n",
    "        self.counter = 0  # count the number of times the validation accuracy not improving\n",
    "        self.min_validation_loss = np.inf\n",
    "\n",
    "    # return True when encountering _patience_ times decrease in validation loss \n",
    "    def __call__(self, validation_loss, verbose=False):\n",
    "        if ((validation_loss+self.min_delta) < self.min_validation_loss):\n",
    "            self.min_validation_loss = validation_loss\n",
    "            self.counter = 0  # reset the counter if validation loss decreased at least by min_delta\n",
    "        elif ((validation_loss+self.min_delta) > self.min_validation_loss):\n",
    "            self.counter += 1 # increase the counter if validation loss is not decreased by the min_delta\n",
    "            if verbose:\n",
    "                print(f\"  >> now{validation_loss:.3f} > best{self.min_validation_loss:.3f}\")\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_MLM(train_loader, valid_loader, model, criterion, optimizer, scheduler,  epochs):\n",
    "\n",
    "    earlyStop = EarlyStopping(patience= 8, min_delta=-10)\n",
    "\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0\n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad() # Zero your gradients for every batch!\n",
    "            \n",
    "            output = model(batch.to(\"cuda\"))\n",
    "            loss = criterion(output, batch.MLM.to(\"cuda\"))\n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step() # Adjust learning weights\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        if epoch % 20 == 0:\n",
    "            model.eval()\n",
    "            valid_loss = 0\n",
    "            with torch.no_grad():\n",
    "                for batch in valid_loader:\n",
    "                    output = model(batch.to(\"cuda\"))\n",
    "                    loss = criterion(output, batch.MLM.to(\"cuda\"))\n",
    "                    valid_loss += loss.item()\n",
    "                 \n",
    "                    \n",
    "            print(f\"Epoch: {epoch:4d}/{epochs} with lr {scheduler.get_last_lr()[0]:.9f}, Train Loss: {np.sqrt(running_loss/len(train_loader))}, Valid Loss: {np.sqrt(valid_loss/len(valid_loader))}\")\n",
    "            \n",
    "\n",
    "            if earlyStop(valid_loss, verbose=True):\n",
    "                break\n",
    "\n",
    "            scheduler.step()    \n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_HLM(train_loader, valid_loader, model, criterion, optimizer, scheduler,  epochs):\n",
    "\n",
    "    earlyStop = EarlyStopping(patience= 8, min_delta=-10)\n",
    "\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0\n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad() # Zero your gradients for every batch!\n",
    "            \n",
    "            output = model(batch.to(\"cuda\"))\n",
    "            loss = criterion(output, batch.HLM.to(\"cuda\"))\n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step() # Adjust learning weights\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        if epoch % 20 == 0:\n",
    "            model.eval()\n",
    "            valid_loss = 0\n",
    "            with torch.no_grad():\n",
    "                for batch in valid_loader:\n",
    "                    output = model(batch.to(\"cuda\"))\n",
    "                    loss = criterion(output, batch.HLM.to(\"cuda\"))\n",
    "                    valid_loss += loss.item()\n",
    "                 \n",
    "                    \n",
    "            print(f\"Epoch: {epoch:4d}/{epochs} with lr {scheduler.get_last_lr()[0]:.9f}, Train Loss: {np.sqrt(running_loss/len(train_loader))}, Valid Loss: {np.sqrt(valid_loss/len(valid_loader))}\")\n",
    "            \n",
    "\n",
    "            if earlyStop(valid_loss, verbose=True):\n",
    "                break\n",
    "\n",
    "            scheduler.step()    \n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter\n",
    "CFG = {'BATCH_SIZE': 256,\n",
    "       'EPOCHS': 8000,\n",
    "       'HIDDEN_SIZE': 1024,\n",
    "       'OUTPUT_SIZE': 1,\n",
    "       'DROPOUT_RATE': 0.8,\n",
    "       'LEARNING_RATE': 0.0001}\n",
    "\n",
    "BERT_param = {\"BERT_out_dim\": 100,\n",
    "              \"projection_dim\": 100+len(given_features),\n",
    "              \"out_dim\" : 1}  # {\"BERT_out_dim\" : bert 인코딩 출력 차원 , \"projection_dim\":  mlp에 들어가는 fc 레이어의 차원 (인코딩 + tabular features), \"out_dim\": 최종 출력 차원}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = KFold_pl_DataModule()\n",
    "model_MLM = ChemBERT(BERT_out_dim = BERT_param[\"BERT_out_dim\"] , projection_dim= BERT_param[\"projection_dim\"] , out_dim = BERT_param[\"out_dim\"] ).to(\"cuda\")\n",
    "model_HLM = ChemBERT(BERT_out_dim = BERT_param[\"BERT_out_dim\"] , projection_dim= BERT_param[\"projection_dim\"] , out_dim = BERT_param[\"out_dim\"] ).to(\"cuda\")\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer_MLM = torch.optim.Adam(model_MLM.parameters(), lr=CFG['LEARNING_RATE'])\n",
    "scheduler_MLM = torch.optim.lr_scheduler.LambdaLR(optimizer = optimizer_MLM, lr_lambda= lambda epoch : 0.95**(epoch))\n",
    "\n",
    "optimizer_HLM = torch.optim.Adam(model_HLM.parameters(), lr=CFG['LEARNING_RATE'])\n",
    "scheduler_HLM = torch.optim.lr_scheduler.LambdaLR(optimizer = optimizer_HLM, lr_lambda= lambda epoch : 0.95**(epoch))\n",
    "\n",
    "#print(model_MLM, model_HLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_MLM = train_MLM(data.train_dataloader(), data.val_dataloader(), model_MLM, criterion, optimizer_MLM, scheduler_MLM, epochs=CFG[\"EPOCHS\"])\n",
    "model_HLM = train_HLM(data.train_dataloader(), data.val_dataloader(), model_HLM, criterion, optimizer_HLM, scheduler_HLM, epochs=CFG[\"EPOCHS\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(test_loader, model, label_scaler=None):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx, batch in enumerate(test_loader):\n",
    "            print(f\"{idx:3d}th: \", end=\" \")\n",
    "            for d in inputs:``\n",
    "                std = np.std(d.detach().numpy())\n",
    "                print(f\"{std} \", end=\" \")\n",
    "            print()\n",
    "            output = model(batch.to(\"cuda\"))\n",
    "            if label_scaler is not None:\n",
    "                output = label_scaler.inverse_transform(output.cpu())\n",
    "            preds.extend(output.flatten().tolist())\n",
    "    \n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_MLM = inference(test_MLM_loader, model_MLM)\n",
    "predictions_HLM = inference(test_HLM_loader, model_HLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('../input/sample_submission.csv')\n",
    "submission['MLM'] = predictions_MLM\n",
    "submission['HLM'] = predictions_HLM\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('../output/submission.csv', index=False)\n",
    "submission.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "smils",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
